{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e1b2cd-68a7-4852-b597-305881207c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training epoch took: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"  Training epoch took: {:}\".format(format_time(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e55034-0d6a-4f12-9850-33036fac7819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "\"\"\"MovieLens dataset\"\"\"\n",
    "\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "\n",
    "import dgl \n",
    "from dgl.data.utils import download, extract_archive, get_download_dir\n",
    "from refex import extract_refex_feature\n",
    "import utils\n",
    "\n",
    "_urls = {\n",
    "    'ml-100k' : 'http://files.grouplens.org/datasets/movielens/ml-100k.zip',\n",
    "    'ml-1m' : 'http://files.grouplens.org/datasets/movielens/ml-1m.zip',\n",
    "}\n",
    "\n",
    "GENRES_ML_100K =\\\n",
    "    ['unknown', 'Action', 'Adventure', 'Animation',\n",
    "     'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "     'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "     'Thriller', 'War', 'Western']\n",
    "GENRES_ML_1M = GENRES_ML_100K[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df84242a-d6e0-415c-b6cd-62c77dc96cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MovieLens(object):\n",
    "    \"\"\"MovieLens dataset used by GCMC model\n",
    "    \"\"\"\n",
    "    def __init__(self, data_name, testing=False, \n",
    "                 test_ratio=0.1, valid_ratio=0.2):\n",
    "        # self._data_name = data_name\n",
    "\n",
    "        # # 1. download and extract\n",
    "        # download_dir = get_download_dir()\n",
    "        # self._dir = os.path.join(download_dir, data_name, data_name)\n",
    "        # if not os.path.exists(self._dir):\n",
    "        #     zip_file_path = '{}/{}.zip'.format(download_dir, data_name)\n",
    "        #     download(_urls[data_name], path=zip_file_path)\n",
    "        #     extract_archive(zip_file_path, '{}/{}'.format(download_dir, data_name))\n",
    "        \n",
    "        # print(\"Starting processing {} ...\".format(self._data_name))\n",
    "\n",
    "        # # 2. load rating data\n",
    "        # if self._data_name == 'ml-100k':\n",
    "        #     train_rating_data = self._load_raw_rates(os.path.join(self._dir, 'u1.base'), '\\t')\n",
    "        #     test_rating_data = self._load_raw_rates(os.path.join(self._dir, 'u1.test'), '\\t')\n",
    "        #     all_rating_data = pd.concat([train_rating_data, test_rating_data])\n",
    "        # elif self._data_name == 'ml-1m':\n",
    "        #     all_rating_data = self._load_raw_rates(os.path.join(self._dir, 'ratings.dat'), '::')\n",
    "        #     num_test = int(np.ceil(all_rating_data.shape[0] * test_ratio))\n",
    "        #     shuffled_idx = np.random.permutation(all_rating_data.shape[0])\n",
    "        #     test_rating_data = all_rating_data.iloc[shuffled_idx[: num_test]]\n",
    "        #     train_rating_data = all_rating_data.iloc[shuffled_idx[num_test: ]]\n",
    "        # else:\n",
    "        #     raise NotImplementedError\n",
    "        # num_valid = int(np.ceil(train_rating_data.shape[0] * valid_ratio))\n",
    "        # shuffled_idx = np.random.permutation(train_rating_data.shape[0])\n",
    "        # valid_rating_data = train_rating_data.iloc[shuffled_idx[: num_valid]]\n",
    "        # if not testing:\n",
    "        #     train_rating_data = train_rating_data.iloc[shuffled_idx[num_valid:]]\n",
    "\n",
    "        # self._rating = np.sort(np.unique(all_rating_data[\"rating\"].values))\n",
    "        \n",
    "        # print(\"All rating pairs : {}\".format(all_rating_data.shape[0]))\n",
    "        # # print(\"\\tAll train rating pairs : {}\".format(self.all_train_rating_data.shape[0]))\n",
    "        # print(\"\\tTrain rating pairs : {}\".format(train_rating_data.shape[0]))\n",
    "        # print(\"\\tValid rating pairs : {}\".format(valid_rating_data.shape[0]))\n",
    "        # print(\"\\tTest rating pairs  : {}\".format(test_rating_data.shape[0]))\n",
    "\n",
    "        # # 2. load user and movie data, and drop those unseen in rating_data\n",
    "        # user_data = self._load_raw_user_data()\n",
    "        # movie_data = self._load_raw_movie_data()\n",
    "        # user_data = self._drop_unseen_nodes(data_df=user_data,\n",
    "        #                                     col_name=\"id\",\n",
    "        #                                     reserved_ids_set=set(all_rating_data[\"user_id\"].values))\n",
    "        # movie_data = self._drop_unseen_nodes(data_df=movie_data,\n",
    "        #                                     col_name=\"id\",\n",
    "        #                                     reserved_ids_set=set(all_rating_data[\"movie_id\"].values))\n",
    "\n",
    "        # # 3. set user and movie feature to None\n",
    "        # user_feature = None\n",
    "        # movie_feature = None\n",
    "\n",
    "        # # 4. generate rating pairs\n",
    "        # # Map user/movie to the global id\n",
    "        # self._global_user_id_map = {ele: i for i, ele in enumerate(user_data['id'])}\n",
    "        # self._global_movie_id_map = {ele: i for i, ele in enumerate(movie_data['id'])}\n",
    "        # print('Total user number = {}, movie number = {}'.format(len(self._global_user_id_map),\n",
    "        #                                                          len(self._global_movie_id_map)))\n",
    "        # self._num_user = len(self._global_user_id_map)\n",
    "        # self._num_movie = len(self._global_movie_id_map)\n",
    "\n",
    "        # # pair value is idx rather than id, and rating value starts from 1.0\n",
    "        # # self.all_train_rating_pairs, self.all_train_rating_values = self._generate_pair_value(self.all_train_rating_data)\n",
    "        # train_u_indices, train_v_indices, train_labels = self._generate_pair_value(train_rating_data)\n",
    "        # val_u_indices, val_v_indices, val_labels = self._generate_pair_value(valid_rating_data)\n",
    "        # test_u_indices, test_v_indices, test_labels = self._generate_pair_value(test_rating_data)\n",
    "\n",
    "        if data_name == 'ml-100k':\n",
    "            print(\"Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\")\n",
    "            (\n",
    "                u_features, v_features, adj_train, train_labels, train_u_indices, train_v_indices,\n",
    "                val_labels, val_u_indices, val_v_indices, test_labels, test_u_indices, \n",
    "                test_v_indices, class_values\n",
    "            ) = load_official_trainvaltest_split(\n",
    "                'ml-100k', testing, None, None, 1.0\n",
    "            )\n",
    "        elif data_name == 'ml-1m':\n",
    "            data_seed = 1234\n",
    "            datasplit_path = (\n",
    "                'raw_data/' + data_name + '/split_seed' + str(data_seed) + \n",
    "                '.pickle'\n",
    "            )\n",
    "            print(\"Using random dataset split ...\")\n",
    "            (\n",
    "                u_features, v_features, adj_train, train_labels, train_u_indices, train_v_indices,\n",
    "                val_labels, val_u_indices, val_v_indices, test_labels, test_u_indices, \n",
    "                test_v_indices, class_values\n",
    "            ) = create_trainvaltest_split(\n",
    "                'ml-1m', 1234, testing, datasplit_path, True, True, None, \n",
    "                None, 1.0\n",
    "            )\n",
    "            \n",
    "        self._num_user = u_features.shape[0]\n",
    "        self._num_movie = v_features.shape[0]\n",
    "\n",
    "        # reindex u and v, v nodes start after u\n",
    "        train_v_indices += self.num_user\n",
    "        val_v_indices += self.num_user\n",
    "        test_v_indices += self.num_user\n",
    "\n",
    "        self.train_rating_pairs = (th.LongTensor(train_u_indices), th.LongTensor(train_v_indices))\n",
    "        self.valid_rating_pairs = (th.LongTensor(val_u_indices), th.LongTensor(val_v_indices))\n",
    "        self.test_rating_pairs = (th.LongTensor(test_u_indices), th.LongTensor(test_v_indices))\n",
    "        self.train_rating_values = th.FloatTensor(train_labels)\n",
    "        self.valid_rating_values = th.FloatTensor(val_labels)\n",
    "        self.test_rating_values = th.FloatTensor(test_labels)\n",
    "\n",
    "        print(\"\\tTrain rating pairs : {}\".format(len(train_labels)))\n",
    "        print(\"\\tValid rating pairs : {}\".format(len(val_labels)))\n",
    "        print(\"\\tTest rating pairs  : {}\".format(len(test_labels)))\n",
    "\n",
    "        # build dgl graph object, which is homogeneous and bidirectional and contains only training edges\n",
    "        self.train_graph = dgl.graph((th.cat([self.train_rating_pairs[0], self.train_rating_pairs[1]]), \n",
    "                                      th.cat([self.train_rating_pairs[1], self.train_rating_pairs[0]])))\n",
    "        self.train_graph.edata['etype'] = th.cat([self.train_rating_values, self.train_rating_values]).to(th.long)\n",
    "                    \n",
    "        # # add refex feature\n",
    "        # refex_feature = extract_refex_feature(self.train_graph)\n",
    "        # print(\"refex feature shape: {}\".format(refex_feature.numpy().shape))\n",
    "        # self.train_graph.ndata['refex'] = refex_feature\n",
    "\n",
    "        # # add gdv feature\n",
    "        # gdv_feature = np.loadtxt('./{}.gdv'.format(data_name), dtype=np.float32)\n",
    "        # print(\"gdv feature shape: {}\".format(gdv_feature.shape))\n",
    "        # gdv_feature = utils.MinMaxScaling(gdv_feature, axis=0)\n",
    "        # self.train_graph.ndata['gdv'] = th.from_numpy(gdv_feature)\n",
    "\n",
    "    @property\n",
    "    def num_rating(self):\n",
    "        return self._rating.size\n",
    "\n",
    "    @property\n",
    "    def num_user(self):\n",
    "        return self._num_user\n",
    "\n",
    "    @property\n",
    "    def num_movie(self):\n",
    "        return self._num_movie\n",
    "\n",
    "    def _load_raw_user_data(self):\n",
    "        \"\"\"In MovieLens, the user attributes file have the following formats:\n",
    "\n",
    "        ml-100k:\n",
    "        user id | age | gender | occupation | zip code\n",
    "\n",
    "        ml-1m:\n",
    "        UserID::Gender::Age::Occupation::Zip-code\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        user_data : pd.DataFrame\n",
    "        \"\"\"\n",
    "        if self._data_name == 'ml-100k':\n",
    "            user_data = pd.read_csv(os.path.join(self._dir, 'u.user'), sep='|', header=None,\n",
    "                                    names=['id', 'age', 'gender', 'occupation', 'zip_code'], engine='python')\n",
    "        elif self._data_name == 'ml-1m':\n",
    "            user_data = pd.read_csv(os.path.join(self._dir, 'users.dat'), sep='::', header=None,\n",
    "                                    names=['id', 'gender', 'age', 'occupation', 'zip_code'], engine='python')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return user_data\n",
    "\n",
    "    def _load_raw_movie_data(self):\n",
    "        \"\"\"In MovieLens, the movie attributes may have the following formats:\n",
    "\n",
    "        In ml-100k:\n",
    "\n",
    "        movie id | movie title | release date | video release date | IMDb URL | [genres]\n",
    "\n",
    "        In ml-1m, ml-10m:\n",
    "\n",
    "        MovieID::Title (Release Year)::Genres\n",
    "\n",
    "        Also, Genres are separated by |, e.g., Adventure|Animation|Children|Comedy|Fantasy\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        movie_data : pd.DataFrame\n",
    "            For ml-100k, the column name is ['id', 'title', 'release_date', 'video_release_date', 'url'] + [GENRES (19)]]\n",
    "            For ml-1m, the column name is ['id', 'title'] + [GENRES (18/20)]]\n",
    "        \"\"\"\n",
    "        if self._data_name == 'ml-100k':\n",
    "            GENRES = GENRES_ML_100K\n",
    "        elif self._data_name == 'ml-1m':\n",
    "            GENRES = GENRES_ML_1M\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if self._data_name == 'ml-100k':\n",
    "            file_path = os.path.join(self._dir, 'u.item')\n",
    "            movie_data = pd.read_csv(file_path, sep='|', header=None,\n",
    "                                          names=['id', 'title', 'release_date', 'video_release_date', 'url'] + GENRES,\n",
    "                                          engine='python')\n",
    "        elif self._data_name == 'ml-1m':\n",
    "            file_path = os.path.join(self._dir, 'movies.dat')\n",
    "            movie_data = pd.read_csv(file_path, sep='::', header=None,\n",
    "                                     names=['id', 'title', 'genres'], engine='python')\n",
    "            genre_map = {ele: i for i, ele in enumerate(GENRES)}\n",
    "            genre_map['Children\\'s'] = genre_map['Children']\n",
    "            genre_map['Childrens'] = genre_map['Children']\n",
    "            movie_genres = np.zeros(shape=(movie_data.shape[0], len(GENRES)), dtype=np.float32)\n",
    "            for i, genres in enumerate(movie_data['genres']):\n",
    "                for ele in genres.split('|'):\n",
    "                    if ele in genre_map:\n",
    "                        movie_genres[i, genre_map[ele]] = 1.0\n",
    "                    else:\n",
    "                        print('genres not found, filled with unknown: {}'.format(genres))\n",
    "                        movie_genres[i, genre_map['unknown']] = 1.0\n",
    "            for idx, genre_name in enumerate(GENRES):\n",
    "                assert idx == genre_map[genre_name]\n",
    "                movie_data[genre_name] = movie_genres[:, idx]\n",
    "            movie_data = movie_data.drop(columns=[\"genres\"])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return movie_data\n",
    "\n",
    "    def _load_raw_rates(self, file_path, sep):\n",
    "        \"\"\"In MovieLens, the rates have the following format\n",
    "\n",
    "        ml-100k\n",
    "        user id \\t movie id \\t rating \\t timestamp\n",
    "\n",
    "        ml-1m/10m\n",
    "        UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "        timestamp is unix timestamp and can be converted by pd.to_datetime(X, unit='s')\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        rating_data : pd.DataFrame\n",
    "        \"\"\"\n",
    "        rating_data = pd.read_csv(\n",
    "            file_path, sep=sep, header=None,\n",
    "            names=['user_id', 'movie_id', 'rating', 'timestamp'],\n",
    "            dtype={'user_id': np.int32, 'movie_id' : np.int32,\n",
    "                   'ratings': np.float32, 'timestamp': np.int64}, engine='python')\n",
    "        return rating_data\n",
    "\n",
    "    def _drop_unseen_nodes(self, data_df, col_name, reserved_ids_set):\n",
    "        data_df = data_df[data_df[col_name].isin(reserved_ids_set)]\n",
    "        data_df.reset_index(drop=True, inplace=True)\n",
    "        return data_df\n",
    "\n",
    "    def _generate_pair_value(self, rating_data):\n",
    "        rating_pairs = (np.array([self._global_user_id_map[ele] for ele in rating_data[\"user_id\"]],\n",
    "                                 dtype=np.int32),\n",
    "                        np.array([self._global_movie_id_map[ele] for ele in rating_data[\"movie_id\"]],\n",
    "                                 dtype=np.int32))\n",
    "        # label ranges from 0. to 4.\n",
    "        rating_values = rating_data[\"rating\"].values.astype(np.float32) - 1.\n",
    "        return rating_pairs[0], rating_pairs[1], rating_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "398e2e59-021d-461f-ab78-d2294b976034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# For automatic dataset downloading\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df99d352-36d8-4cca-8150-724f4d9c732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data(data):\n",
    "    \"\"\"\n",
    "    Map data to proper indices in case they are not in a continues [0, N) range\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.int32 arrays\n",
    "    Returns\n",
    "    -------\n",
    "    mapped_data : np.int32 arrays\n",
    "    n : length of mapped_data\n",
    "    \"\"\"\n",
    "    uniq = list(set(data))\n",
    "\n",
    "    id_dict = {old: new for new, old in enumerate(sorted(uniq))}\n",
    "    data = np.array([id_dict[x] for x in data])\n",
    "    n = len(uniq)\n",
    "\n",
    "    return data, id_dict, n\n",
    "\n",
    "def download_dataset(dataset, files, data_dir):\n",
    "    \"\"\" Downloads dataset if files are not present. \"\"\"\n",
    "\n",
    "    if not np.all([os.path.isfile(data_dir + f) for f in files]):\n",
    "        url = \"http://files.grouplens.org/datasets/movielens/\" + dataset.replace('_', '-') + '.zip'\n",
    "        request = urlopen(url)\n",
    "\n",
    "        print('Downloading %s dataset' % dataset)\n",
    "\n",
    "        if dataset in ['ml-100k', 'ml-1m']:\n",
    "            target_dir = 'raw_data/' + dataset.replace('_', '-')\n",
    "        elif dataset == 'ml-10m':\n",
    "            target_dir = 'raw_data/' + 'ml-10M100K'\n",
    "        else:\n",
    "            raise ValueError('Invalid dataset option %s' % dataset)\n",
    "\n",
    "        with ZipFile(BytesIO(request.read())) as zip_ref:\n",
    "            zip_ref.extractall('raw_data/')\n",
    "\n",
    "        os.rename(target_dir, data_dir)\n",
    "        #shutil.rmtree(target_dir)\n",
    "\n",
    "def load_data(fname, seed=1234, verbose=True):\n",
    "    \"\"\" Loads dataset and creates adjacency matrix\n",
    "    and feature matrix\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str, dataset\n",
    "    seed: int, dataset shuffling seed\n",
    "    verbose: to print out statements or not\n",
    "    Returns\n",
    "    -------\n",
    "    num_users : int\n",
    "        Number of users and items respectively\n",
    "    num_items : int\n",
    "    u_nodes : np.int32 arrays\n",
    "        User indices\n",
    "    v_nodes : np.int32 array\n",
    "        item (movie) indices\n",
    "    ratings : np.float32 array\n",
    "        User/item ratings s.t. ratings[k] is the rating given by user u_nodes[k] to\n",
    "        item v_nodes[k]. Note that that the all pairs u_nodes[k]/v_nodes[k] are unique, but\n",
    "        not necessarily all u_nodes[k] or all v_nodes[k] separately.\n",
    "    u_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "    v_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "    seed: int,\n",
    "        For datashuffling seed with pythons own random.shuffle, as in CF-NADE.\n",
    "    \"\"\"\n",
    "\n",
    "    u_features = None\n",
    "    v_features = None\n",
    "\n",
    "    print('Loading dataset', fname)\n",
    "\n",
    "    data_dir = 'raw_data/' + fname\n",
    "\n",
    "    if fname == 'ml-100k':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['/u.data', '/u.item', '/u.user']\n",
    "\n",
    "        download_dataset(fname, files, data_dir)\n",
    "\n",
    "        sep = '\\t'\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "        data = pd.read_csv(\n",
    "            filename, sep=sep, header=None,\n",
    "            names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        data_array = data.values.tolist()\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "        ratings = ratings.astype(np.float64)\n",
    "\n",
    "        # Movie features (genres)\n",
    "        sep = r'|'\n",
    "        movie_file = data_dir + files[1]\n",
    "        movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "                         'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                         'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                         'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                         'Thriller', 'War', 'Western']\n",
    "        movie_df = pd.read_csv(movie_file, sep=sep, header=None, \n",
    "                               names=movie_headers, encoding='ISO-8859-1')\n",
    "\n",
    "        genre_headers = movie_df.columns.values[6:]\n",
    "        num_genres = genre_headers.shape[0]\n",
    "\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "            # Check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                v_features[v_dict[movie_id], :] = g_vec\n",
    "\n",
    "        # User features\n",
    "\n",
    "        sep = r'|'\n",
    "        users_file = data_dir + files[2]\n",
    "        users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        occupation = set(users_df['occupation'].values.tolist())\n",
    "\n",
    "        gender_dict = {'M': 0., 'F': 1.}\n",
    "        occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "        num_feats = 2 + len(occupation_dict)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user id']\n",
    "            if u_id in u_dict.keys():\n",
    "                # age\n",
    "                u_features[u_dict[u_id], 0] = row['age']\n",
    "                # gender\n",
    "                u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "                # occupation\n",
    "                u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1.\n",
    "\n",
    "        u_features = sp.csr_matrix(u_features)\n",
    "        v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    elif fname == 'ml-1m':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['/ratings.dat', '/movies.dat', '/users.dat']\n",
    "        download_dataset(fname, files, data_dir)\n",
    "\n",
    "        sep = r'\\:\\:'\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int64, 'v_nodes': np.int64,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "        # use engine='python' to ignore warning about switching to python backend when using regexp for sep\n",
    "        data = pd.read_csv(filename, sep=sep, header=None,\n",
    "                           names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], converters=dtypes, engine='python')\n",
    "\n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        data_array = data.values.tolist()\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int64)\n",
    "        ratings = ratings.astype(np.float32)\n",
    "\n",
    "        # Load movie features\n",
    "        movies_file = data_dir + files[1]\n",
    "        movies_headers = ['movie_id', 'title', 'genre']\n",
    "        movies_df = pd.read_csv(movies_file, sep=sep, header=None,\n",
    "                                names=movies_headers, engine='python', encoding='ISO-8859-1')\n",
    "\n",
    "        # Extracting all genres\n",
    "        genres = []\n",
    "        for s in movies_df['genre'].values:\n",
    "            genres.extend(s.split('|'))\n",
    "\n",
    "        genres = list(set(genres))\n",
    "        num_genres = len(genres)\n",
    "\n",
    "        genres_dict = {g: idx for idx, g in enumerate(genres)}\n",
    "\n",
    "        # Creating 0 or 1 valued features for all genres\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, s in zip(movies_df['movie_id'].values.tolist(), movies_df['genre'].values.tolist()):\n",
    "            # Check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                gen = s.split('|')\n",
    "                for g in gen:\n",
    "                    v_features[v_dict[movie_id], genres_dict[g]] = 1.\n",
    "\n",
    "        # Load user features\n",
    "        users_file = data_dir + files[2]\n",
    "        users_headers = ['user_id', 'gender', 'age', 'occupation', 'zip-code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        # Extracting all features\n",
    "        cols = users_df.columns.values[1:]\n",
    "\n",
    "        cntr = 0\n",
    "        feat_dicts = []\n",
    "        for header in cols:\n",
    "            d = dict()\n",
    "            feats = np.unique(users_df[header].values).tolist()\n",
    "            d.update({f: i for i, f in enumerate(feats, start=cntr)})\n",
    "            feat_dicts.append(d)\n",
    "            cntr += len(d)\n",
    "\n",
    "        num_feats = sum(len(d) for d in feat_dicts)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user_id']\n",
    "            if u_id in u_dict.keys():\n",
    "                for k, header in enumerate(cols):\n",
    "                    u_features[u_dict[u_id], feat_dicts[k][row[header]]] = 1.\n",
    "\n",
    "        u_features = sp.csr_matrix(u_features)\n",
    "        v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    elif fname == 'ml-10m':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['/ratings.dat']\n",
    "        download_dataset(fname, files, data_dir)\n",
    "\n",
    "        sep = r'\\:\\:'\n",
    "\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int64, 'v_nodes': np.int64,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "        # use engine='python' to ignore warning about switching to python backend when using regexp for sep\n",
    "        data = pd.read_csv(filename, sep=sep, header=None,\n",
    "                           names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], converters=dtypes, engine='python')\n",
    "\n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        data_array = data.values.tolist()\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int64)\n",
    "        ratings = ratings.astype(np.float32)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Dataset name not recognized: ' + fname)\n",
    "\n",
    "    if verbose:\n",
    "        print('Number of users = %d' % num_users)\n",
    "        print('Number of items = %d' % num_items)\n",
    "        print('Number of links = %d' % ratings.shape[0])\n",
    "        print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "    return num_users, num_items, u_nodes_ratings, v_nodes_ratings, ratings, u_features, v_features\n",
    "\n",
    "# 'ml-1m'에서 사용\n",
    "def create_trainvaltest_split(seed=1234, testing=False, datasplit_path=None, \n",
    "                              datasplit_from_file=False, verbose=True, rating_map=None, \n",
    "                              post_rating_map=None, ratio=1.0):\n",
    "    \"\"\"\n",
    "    Splits data set into train/val/test sets from full bipartite adjacency matrix. Shuffling of dataset is done in\n",
    "    load_data function.\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    if datasplit_from_file and os.path.isfile(datasplit_path):\n",
    "        print('Reading dataset splits from file...')\n",
    "        with open(datasplit_path, 'rb') as f:\n",
    "            num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = pkl.load(f)\n",
    "\n",
    "        if verbose:\n",
    "            print('Number of users = %d' % num_users)\n",
    "            print('Number of items = %d' % num_items)\n",
    "            print('Number of links = %d' % ratings.shape[0])\n",
    "            print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "    else:\n",
    "        num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = load_data(dataset, seed=seed,\n",
    "                                                                                            verbose=verbose)\n",
    "\n",
    "        with open(datasplit_path, 'wb') as f:\n",
    "            pkl.dump([num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features], f)\n",
    "\n",
    "    if rating_map is not None:\n",
    "        for i, x in enumerate(ratings):\n",
    "            ratings[i] = rating_map[x]\n",
    "\n",
    "    neutral_rating = -1\n",
    "\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges\n",
    "    num_test = int(np.ceil(ratings.shape[0] * 0.1))\n",
    "    if dataset == 'ml-100k':\n",
    "        num_val = int(np.ceil(ratings.shape[0] * 0.9 * 0.05))\n",
    "    else:\n",
    "        num_val = int(np.ceil(ratings.shape[0] * 0.9 * 0.05))\n",
    "\n",
    "    num_train = ratings.shape[0] - num_val - num_test\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    train_idx = idx_nonzero[0:int(num_train*ratio)]\n",
    "    val_idx = idx_nonzero[num_train:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    train_pairs_idx = pairs_nonzero[0:int(num_train*ratio)]\n",
    "    val_pairs_idx = pairs_nonzero[num_train:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    if post_rating_map is None:\n",
    "        rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    else:\n",
    "        rating_mx_train[train_idx] = np.array([post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values\n",
    "\n",
    "# 'ml-100k'에서 사용\n",
    "def load_official_trainvaltest_split(dataset, testing=False, rating_map=None, post_rating_map=None, ratio=1.0):\n",
    "    \"\"\"\n",
    "    Loads official train/test split and uses 10% of training samples for validaiton\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix. Assumes flattening happens everywhere in row-major fashion.\n",
    "    \"\"\"\n",
    "\n",
    "    sep = '\\t'\n",
    "\n",
    "    # Check if files exist and download otherwise\n",
    "    files = ['/u1.base', '/u1.test', '/u.item', '/u.user']\n",
    "    fname = dataset\n",
    "    data_dir = 'raw_data/' + fname\n",
    "\n",
    "    download_dataset(fname, files, data_dir)\n",
    "\n",
    "    dtypes = {\n",
    "        'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "        'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "    filename_train = 'raw_data/' + dataset + '/u1.base'\n",
    "    filename_test = 'raw_data/' + dataset + '/u1.test'\n",
    "\n",
    "    data_train = pd.read_csv(\n",
    "        filename_train, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "    data_test = pd.read_csv(\n",
    "        filename_test, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "    data_array_train = data_train.values.tolist()\n",
    "    data_array_train = np.array(data_array_train)\n",
    "    data_array_test = data_test.values.tolist()\n",
    "    data_array_test = np.array(data_array_test)\n",
    "\n",
    "    if ratio < 1.0:\n",
    "        data_array_train = data_array_train[data_array_train[:, -1].argsort()[:int(ratio*len(data_array_train))]]\n",
    "\n",
    "    data_array = np.concatenate([data_array_train, data_array_test], axis=0)\n",
    "\n",
    "    u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "    v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "    ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "    if rating_map is not None:\n",
    "        for i, x in enumerate(ratings):\n",
    "            ratings[i] = rating_map[x]\n",
    "\n",
    "    u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "    v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "    u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "    ratings = ratings.astype(np.float64)\n",
    "\n",
    "    u_nodes = u_nodes_ratings\n",
    "    v_nodes = v_nodes_ratings\n",
    "\n",
    "    neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "    # assumes that ratings_train contains at least one example of every rating type\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "\n",
    "    for i in range(len(u_nodes)):\n",
    "        assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges, see cf-nade code\n",
    "\n",
    "    num_train = data_array_train.shape[0]\n",
    "    num_test = data_array_test.shape[0]\n",
    "    num_val = int(np.ceil(num_train * 0.2))\n",
    "    num_train = num_train - num_val\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    for i in range(len(ratings)):\n",
    "        assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "    idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "    pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "    pairs_nonzero_test = pairs_nonzero[num_train+num_val:]\n",
    "\n",
    "    # Internally shuffle training set (before splitting off validation set)\n",
    "    rand_idx = list(range(len(idx_nonzero_train)))\n",
    "    np.random.seed(1234)\n",
    "    np.random.shuffle(rand_idx)\n",
    "    idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "    pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "    idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "    pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "    val_idx = idx_nonzero[0:num_val]\n",
    "    train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    assert(len(test_idx) == num_test)\n",
    "\n",
    "    val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "    train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "    \n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    if post_rating_map is None:\n",
    "        rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    else:\n",
    "        rating_mx_train[train_idx] = np.array([post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    if dataset =='ml-100k':\n",
    "\n",
    "        # movie features (genres)\n",
    "        sep = r'|'\n",
    "        movie_file = 'raw_data/' + dataset + '/u.item'\n",
    "        movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "                         'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                         'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                         'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                         'Thriller', 'War', 'Western']\n",
    "        movie_df = pd.read_csv(movie_file, sep=sep, header=None, \n",
    "                               names=movie_headers, encoding='ISO-8859-1')\n",
    "\n",
    "        genre_headers = movie_df.columns.values[6:]\n",
    "        num_genres = genre_headers.shape[0]\n",
    "\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "            # check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                v_features[v_dict[movie_id], :] = g_vec\n",
    "\n",
    "        # user features\n",
    "\n",
    "        sep = r'|'\n",
    "        users_file = 'raw_data/' + dataset + '/u.user'\n",
    "        users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        occupation = set(users_df['occupation'].values.tolist())\n",
    "\n",
    "        age = users_df['age'].values\n",
    "        age_max = age.max()\n",
    "\n",
    "        gender_dict = {'M': 0., 'F': 1.}\n",
    "        occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "        num_feats = 2 + len(occupation_dict)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user id']\n",
    "            if u_id in u_dict.keys():\n",
    "                # age\n",
    "                u_features[u_dict[u_id], 0] = row['age'] / np.float(age_max)\n",
    "                # gender\n",
    "                u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "                # occupation\n",
    "                u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1.\n",
    "\n",
    "    elif dataset == 'ml-1m':\n",
    "\n",
    "        # load movie features\n",
    "        movies_file = 'raw_data/' + dataset + '/movies.dat'\n",
    "\n",
    "        movies_headers = ['movie_id', 'title', 'genre']\n",
    "        movies_df = pd.read_csv(movies_file, sep=sep, header=None,\n",
    "                                names=movies_headers, engine='python')\n",
    "\n",
    "        # extracting all genres\n",
    "        genres = []\n",
    "        for s in movies_df['genre'].values:\n",
    "            genres.extend(s.split('|'))\n",
    "\n",
    "        genres = list(set(genres))\n",
    "        num_genres = len(genres)\n",
    "\n",
    "        genres_dict = {g: idx for idx, g in enumerate(genres)}\n",
    "\n",
    "        # creating 0 or 1 valued features for all genres\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, s in zip(movies_df['movie_id'].values.tolist(), movies_df['genre'].values.tolist()):\n",
    "            # check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                gen = s.split('|')\n",
    "                for g in gen:\n",
    "                    v_features[v_dict[movie_id], genres_dict[g]] = 1.\n",
    "\n",
    "        # load user features\n",
    "        users_file = 'raw_data/' + dataset + '/users.dat'\n",
    "        users_headers = ['user_id', 'gender', 'age', 'occupation', 'zip-code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        # extracting all features\n",
    "        cols = users_df.columns.values[1:]\n",
    "\n",
    "        cntr = 0\n",
    "        feat_dicts = []\n",
    "        for header in cols:\n",
    "            d = dict()\n",
    "            feats = np.unique(users_df[header].values).tolist()\n",
    "            d.update({f: i for i, f in enumerate(feats, start=cntr)})\n",
    "            feat_dicts.append(d)\n",
    "            cntr += len(d)\n",
    "\n",
    "        num_feats = sum(len(d) for d in feat_dicts)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user_id']\n",
    "            if u_id in u_dict.keys():\n",
    "                for k, header in enumerate(cols):\n",
    "                    u_features[u_dict[u_id], feat_dicts[k][row[header]]] = 1.\n",
    "    else:\n",
    "        raise ValueError('Invalid dataset option %s' % dataset)\n",
    "\n",
    "    u_features = sp.csr_matrix(u_features)\n",
    "    v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    print(\"User features shape: \"+str(u_features.shape))\n",
    "    print(\"Item features shape: \"+str(v_features.shape))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92ac58-d531-44f6-8393-c2c1dfe4e71d",
   "metadata": {},
   "source": [
    "# 1. Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3425856-90c9-4cac-9616-9346a01d084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\n",
      "User features shape: (943, 23)\n",
      "Item features shape: (1682, 18)\n",
      "\tTrain rating pairs : 80000\n",
      "\tValid rating pairs : 16000\n",
      "\tTest rating pairs  : 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_24500/3020074856.py:569: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  u_features[u_dict[u_id], 0] = row['age'] / np.float(age_max)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset = MovieLens(\"ml-100k\", testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2fabaeb-54d2-44f4-8d40-27e9cb771171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MovieLens at 0x14d0f7bb280>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a04e62-2003-4474-98e9-024156a6ded9",
   "metadata": {},
   "source": [
    "# 2. load_official_trainvaltest_split 함수(ml-100k 파트만 남김)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b69d4a-fadd-45db-9035-aa17c4443d30",
   "metadata": {},
   "source": [
    "- 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ec2f78f-2434-43cd-9ce5-f9789d40f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ml-100k'\n",
    "testing = False\n",
    "rating_map = None\n",
    "post_rating_map = None\n",
    "ratio = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47efa3-5007-46f3-8ed7-73ff4b6f8536",
   "metadata": {},
   "source": [
    "- Download the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc5d8416-3973-4417-9d46-a2aa0374639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = '\\t'\n",
    "\n",
    "# Check if files exist and download otherwise\n",
    "files = ['/u1.base', '/u1.test', '/u.item', '/u.user']\n",
    "fname = dataset\n",
    "data_dir = 'raw_data/' + fname\n",
    "\n",
    "download_dataset(fname, files, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49553470-58f8-4c11-a139-5e6c7be8b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "    'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "filename_train = 'raw_data/' + dataset + '/u1.base'\n",
    "filename_test = 'raw_data/' + dataset + '/u1.test'\n",
    "\n",
    "data_train = pd.read_csv(\n",
    "    filename_train, sep=sep, header=None,\n",
    "    names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "data_test = pd.read_csv(\n",
    "    filename_test, sep=sep, header=None,\n",
    "    names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffda6614-7b10-4dad-a3c0-27c458b002ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_nodes</th>\n",
       "      <th>v_nodes</th>\n",
       "      <th>ratings</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>874965758.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>876893171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>878542960.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>876893119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>889751712.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   u_nodes  v_nodes  ratings    timestamp\n",
       "0        1        1      5.0  874965758.0\n",
       "1        1        2      3.0  876893171.0\n",
       "2        1        3      4.0  878542960.0\n",
       "3        1        4      3.0  876893119.0\n",
       "4        1        5      3.0  889751712.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c85ace-99b9-4275-81d3-48d1b435574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 4)\n",
      "(20000, 4)\n"
     ]
    }
   ],
   "source": [
    "data_array_train = data_train.values.tolist()\n",
    "data_array_train = np.array(data_array_train)\n",
    "data_array_test = data_test.values.tolist()\n",
    "data_array_test = np.array(data_array_test)\n",
    "\n",
    "print(data_array_train.shape)\n",
    "print(data_array_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c251f672-a28f-4f6e-9431-937f6565ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간에 따라 오름차순 정렬 후 비율에 따라 trainset을 나눔\n",
    "if ratio < 1.0:\n",
    "    data_array_train = data_array_train[data_array_train[:, -1].argsort()[:int(ratio*len(data_array_train))]]\n",
    "\n",
    "data_array = np.concatenate([data_array_train, data_array_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1cba2f3-8165-448c-8aba-ee7ab58ae90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11f5360d-0890-4ce4-a107-c76ec2876ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "if rating_map is not None:\n",
    "    for i, x in enumerate(ratings):\n",
    "        ratings[i] = rating_map[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "538e7593-ddc7-4d09-9e62-09d3c85842a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   1,   1, ..., 459, 460, 462])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_nodes_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1343ec85-eee7-4e1b-a60c-f79d842b9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 0번부터 시작하게끔 모든 인덱스를 당기기  ex) 1번 -> 0번 / 456번 -> 455번\n",
    "u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66c39f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "1682\n"
     ]
    }
   ],
   "source": [
    "print(num_users)\n",
    "print(num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cddfb99-468a-4f23-a8a4-5bdc1102c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "ratings = ratings.astype(np.float64)\n",
    "\n",
    "u_nodes = u_nodes_ratings\n",
    "v_nodes = v_nodes_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68e7bdff-b337-4e2c-8376-3041516dcd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0, 4.0, 5.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.unique(ratings)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90c2f9f3-6f75-4db8-a8a7-3d90e23a988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 1682)\n"
     ]
    }
   ],
   "source": [
    "neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "# assumes that ratings_train contains at least one example of every rating type\n",
    "rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "375ef7b1-7bf0-4ea1-ace1-89b08c9f8102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       ...,\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels # user,item의 rating matrix를 -1로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a84f9f4-dbe1-4e50-a519-71923aa13e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13be05b5-bf7d-4516-b474-7ee0398e86c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  2,  3, ..., -1, -1, -1],\n",
       "       [ 3, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       ...,\n",
       "       [ 4, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1,  4, -1, ..., -1, -1, -1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c77ea18-28e3-47b7-8caf-15f15a993fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(u_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5a5ca94-ff15-485f-8853-2deed9d9fcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 4., ..., 3., 3., 5.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70d443d8-c266-4035-89ae-9b1616c1e019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b68a4a89-fd1b-442a-82d1-373711049236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평점이 제대로 들어갔는지 점검\n",
    "for i in range(len(u_nodes)):\n",
    "    assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "labels = labels.reshape([-1])\n",
    "\n",
    "# number of test and validation edges, see cf-nade code\n",
    "\n",
    "num_train = data_array_train.shape[0]\n",
    "num_test = data_array_test.shape[0]\n",
    "num_val = int(np.ceil(num_train * 0.2))\n",
    "num_train = num_train - num_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5cfe6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b0f4d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1586126"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f943d6cd-13ce-4ba3-baab-f11205f58519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n",
      "16000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(num_train)\n",
    "print(num_val)\n",
    "print(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b8278ab-c34f-42c3-9146-9e3f63e3c3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e6a98f5-5b2a-4bb9-af64-239bff444110",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero]) # rating index 번호"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f51034-a452-4c8e-b3ef-a5415df0293b",
   "metadata": {},
   "source": [
    "- trainset/testset 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5c07cb0-fd10-44d0-9fc5-383e025e27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ratings)):\n",
    "    assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "pairs_nonzero_test = pairs_nonzero[num_train+num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed549e7d-0cc6-469c-97f5-f11ccc2663e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validset 생성 전에 trainset을 섞기\n",
    "rand_idx = list(range(len(idx_nonzero_train)))\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(rand_idx)\n",
    "idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "val_idx = idx_nonzero[0:num_val]\n",
    "train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "assert(len(test_idx) == num_test)\n",
    "\n",
    "val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "test_pairs_idx = pairs_nonzero[num_train + num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd0e99ba-ff3c-4a72-a990-381476580f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_test_idx, v_test_idx = test_pairs_idx.transpose() # 2 x 20000으로 변경\n",
    "u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "# create labels\n",
    "train_labels = labels[train_idx]\n",
    "val_labels = labels[val_idx]\n",
    "test_labels = labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e351534-d271-46d0-b864-fec327ea829d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e71cf98b-773f-4406-aef9-4c272f32b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "    v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "    train_labels = np.hstack([train_labels, val_labels])\n",
    "    # for adjacency matrix construction\n",
    "    train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "class_values = np.sort(np.unique(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9963cc16-a783-4a9d-9286-90244c0543bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset의 인접행렬 생성\n",
    "rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "if post_rating_map is None:\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "else:\n",
    "    rating_mx_train[train_idx] = np.array([post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n",
    "    \n",
    "rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381cdccb-e6e1-4ae2-a7f6-6368543b36f0",
   "metadata": {},
   "source": [
    "- Movie Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7fa87fe-78fe-4423-8871-d4367e30d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie features (genres)\n",
    "sep = r'|'\n",
    "movie_file = 'raw_data/' + dataset + '/u.item'\n",
    "movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "                 'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                 'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                 'Thriller', 'War', 'Western']\n",
    "movie_df = pd.read_csv(movie_file, sep=sep, header=None, \n",
    "                       names=movie_headers, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e59a689-5b26-4951-a8ad-b5a79accb33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie id</th>\n",
       "      <th>movie title</th>\n",
       "      <th>release date</th>\n",
       "      <th>video release date</th>\n",
       "      <th>IMDb URL</th>\n",
       "      <th>unknown</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Childrens</th>\n",
       "      <th>...</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie id       movie title release date  video release date  \\\n",
       "0         1  Toy Story (1995)  01-Jan-1995                 NaN   \n",
       "\n",
       "                                            IMDb URL  unknown  Action  \\\n",
       "0  http://us.imdb.com/M/title-exact?Toy%20Story%2...        0       0   \n",
       "\n",
       "   Adventure  Animation  Childrens  ...  Fantasy  Film-Noir  Horror  Musical  \\\n",
       "0          0          1          1  ...        0          0       0        0   \n",
       "\n",
       "   Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
       "0        0        0       0         0    0        0  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "763c9a0a-1ecc-4cef-9b3a-ee1a51431a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_headers = movie_df.columns.values[6:]\n",
    "num_genres = genre_headers.shape[0]\n",
    "\n",
    "v_features = np.zeros((num_items, num_genres), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f119caa-7f6e-4f71-944e-2ed804ec675c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682, 18)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "282e7204-b1b5-4301-9304-d3d23a3e363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_vec : movie_id별 장르 벡터\n",
    "for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "    # check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "    if movie_id in v_dict.keys():\n",
    "        v_features[v_dict[movie_id], :] = g_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6502aca8-41f3-4f68-b67f-30f9eafc058a",
   "metadata": {},
   "source": [
    "- User features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa74c7b3-5bcf-4272-9c94-ee68ff55ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user features\n",
    "sep = r'|'\n",
    "users_file = 'raw_data/' + dataset + '/u.user'\n",
    "users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                       names=users_headers, engine='python')\n",
    "\n",
    "occupation = set(users_df['occupation'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35e20308-b10b-4f5e-94c2-2be6386dfcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = users_df['age'].values\n",
    "age_max = age.max()\n",
    "\n",
    "gender_dict = {'M': 0., 'F': 1.}\n",
    "occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "num_feats = 2 + len(occupation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9043b91b-ca83-410a-ab3e-f3f4137a455b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user feature 유형: age(1), gender(1), occupation(21)   총 23개\n",
    "num_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c1c0e8b-deed-486f-8207-0b02d64f0d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>85711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>43537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>15213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user id  age gender  occupation zip code\n",
       "0        1   24      M  technician    85711\n",
       "1        2   53      F       other    94043\n",
       "2        3   23      M      writer    32067\n",
       "3        4   24      M  technician    43537\n",
       "4        5   33      F       other    15213"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80055a9c-8161-42fd-93f5-b0e7fd14d875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_24500/1528849455.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  u_features[u_dict[u_id], 0] = row['age'] / np.float(age_max)\n"
     ]
    }
   ],
   "source": [
    "u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "for _, row in users_df.iterrows():\n",
    "    u_id = row['user id']\n",
    "    if u_id in u_dict.keys():\n",
    "        # age\n",
    "        u_features[u_dict[u_id], 0] = row['age'] / np.float(age_max)\n",
    "        # gender\n",
    "        u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "        # occupation\n",
    "        u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03f1db78-8c41-4c28-a09c-c1bdc5dd7b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User features shape: (943, 23)\n",
      "Item features shape: (1682, 18)\n"
     ]
    }
   ],
   "source": [
    "u_features = sp.csr_matrix(u_features)\n",
    "v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "print(\"User features shape: \"+str(u_features.shape))\n",
    "print(\"Item features shape: \"+str(v_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade771c9-4df1-42a6-b32c-ae53eb8d06e4",
   "metadata": {},
   "source": [
    "# 3. create_trainvaltest_split (ml-1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe2da0-13a2-4516-a2cc-c07534832c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 train_multi_gpu.py --data_name ml-1m --testing \\\n",
    "                --batch_size 32 --edge_dropout 0. --max_nodes_per_hop 100 --train_epochs 40 \\\n",
    "                --train_log_interval 1000 --valid_log_interval 5 --train_lr_decay_step 20 \\\n",
    "                --gpu 0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c8bd9eca-86a7-4378-83c1-092a4e5c9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ml-1m'\n",
    "seed = 1234\n",
    "testing = False\n",
    "datasplit_path = './data.pickle'\n",
    "verbose = True\n",
    "rating_map = None\n",
    "post_rating_map = None\n",
    "ratio = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eae909aa-fab1-4baa-9ac1-5db3bc01c966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset splits from file...\n",
      "Number of users = 6040\n",
      "Number of items = 3706\n",
      "Number of links = 1000209\n",
      "Fraction of positive links = 0.0447\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Splits data set into train/val/test sets from full bipartite adjacency matrix. \n",
    "Shuffling of dataset is done inload_data function.\n",
    "For each split computes 1-of-num_classes labels. \n",
    "Also computes training adjacency matrix.\n",
    "\"\"\"\n",
    "\n",
    "if os.path.isfile(datasplit_path):\n",
    "    print('Reading dataset splits from file...')\n",
    "    with open(datasplit_path, 'rb') as f:\n",
    "        num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = pkl.load(f)\n",
    "\n",
    "    if verbose:\n",
    "        print('Number of users = %d' % num_users)\n",
    "        print('Number of items = %d' % num_items)\n",
    "        print('Number of links = %d' % ratings.shape[0])\n",
    "        print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "else:\n",
    "    num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = load_data(dataset, seed=seed, verbose=verbose)\n",
    "\n",
    "#     with open(datasplit_path, 'wb') as f:\n",
    "#         pkl.dump([num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b6b64551-effb-47ec-9473-4ae98293c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datasplit_path, 'wb') as f:\n",
    "    pkl.dump([num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d1ff974a-efe5-499a-a7af-6ad2169a538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datasplit_path, 'rb') as f:\n",
    "    num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "314e9a72-a70c-4546-9f65-8c0a1fcb33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rating_map is not None:\n",
    "    for i, x in enumerate(ratings):\n",
    "        ratings[i] = rating_map[x]\n",
    "\n",
    "neutral_rating = -1\n",
    "\n",
    "rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "labels = labels.reshape([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc2f19b9-017c-49af-bd21-7408f911987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of test and validation edges\n",
    "num_test = int(np.ceil(ratings.shape[0] * 0.1))\n",
    "num_val = int(np.ceil(ratings.shape[0] * 0.9 * 0.05))\n",
    "num_train = ratings.shape[0] - num_val - num_test\n",
    "\n",
    "pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "\n",
    "idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "train_idx = idx_nonzero[0:int(num_train*ratio)]\n",
    "val_idx = idx_nonzero[num_train:num_train + num_val]\n",
    "test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "train_pairs_idx = pairs_nonzero[0:int(num_train*ratio)]\n",
    "val_pairs_idx = pairs_nonzero[num_train:num_train + num_val]\n",
    "test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "u_train_idx, v_train_idx = train_pairs_idx.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9f528825-ef16-4e7a-85f2-3840b59d0cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "train_labels = labels[train_idx]\n",
    "val_labels = labels[val_idx]\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "if testing:\n",
    "    u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "    v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "    train_labels = np.hstack([train_labels, val_labels])\n",
    "    # for adjacency matrix construction\n",
    "    train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "class_values = np.sort(np.unique(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "48042ed6-1478-4a48-916c-229eec0247a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training adjacency matrix\n",
    "rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "if post_rating_map is None:\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "else:\n",
    "    rating_mx_train[train_idx] = np.array([post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n",
    "rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a11e0a02-5066-42df-b093-76ce8c8d6722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6040x3706 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 855178 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_mx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e0903421-8d80-4a78-8c2d-1f75e41ebd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6040x3469 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 24160 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d9772-7390-4d70-a51f-2c48f1203279",
   "metadata": {},
   "outputs": [],
   "source": [
    "return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "    val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eac122-bcd3-4eb2-8797-7fe405fb32fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f88eb9-8a10-498b-99f7-41826d23ebb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a6e96-1083-4f3e-a5d7-6a9ce3ee42ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea0eeb86-7ba7-4fa0-b705-e82d0c601095",
   "metadata": {},
   "source": [
    "# 4. MovieLens 클래스 분석(ml-100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "591ed2fa-969c-4819-97fb-badd8973d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\n",
      "User features shape: (943, 23)\n",
      "Item features shape: (1682, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_24500/2613257520.py:570: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  u_features[u_dict[u_id], 0] = row['age'] / np.float(age_max)\n"
     ]
    }
   ],
   "source": [
    "# MovieLens(\"ml-100k\", testing=True)\n",
    "\n",
    "data_name = 'ml-100k'\n",
    "testing = False\n",
    "test_ratio = 0.1\n",
    "valid_ratio = 0.2\n",
    "    \n",
    "if data_name == 'ml-100k':\n",
    "    print(\"Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\")\n",
    "    (\n",
    "        u_features, v_features, adj_train, \n",
    "        train_labels, train_u_indices, train_v_indices,\n",
    "        val_labels, val_u_indices, val_v_indices, \n",
    "        test_labels, test_u_indices, test_v_indices, \n",
    "        class_values\n",
    "    ) = load_official_trainvaltest_split(\n",
    "        'ml-100k', testing, None, None, 1.0\n",
    "    )\n",
    "\n",
    "_num_user = u_features.shape[0]\n",
    "_num_movie = v_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d865876-102f-453c-986c-c15fb7240f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "1682\n"
     ]
    }
   ],
   "source": [
    "print(_num_user)\n",
    "print(_num_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbcf225e-9f5e-445b-88b3-17e29034916a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 23)\n",
      "(1682, 18)\n"
     ]
    }
   ],
   "source": [
    "print(u_features.shape)\n",
    "print(v_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d2e45cb-d851-4f5c-9ea2-ec1b46fc02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# property 할당(동일한 이름으로 사용)\n",
    "# num_rating = _rating.size  #GCMC에서 사용\n",
    "num_user = _num_user\n",
    "num_movie = _num_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b77b80ce-4a9e-4209-a61d-727daffa2a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166 250 204 ... 183 945 281]\n",
      "[ 429  596  187 ...  209 1024  287]\n",
      "[  5   9  11 ... 933   9 681]\n",
      "943\n"
     ]
    }
   ],
   "source": [
    "print(train_v_indices)\n",
    "print(val_v_indices)\n",
    "print(test_v_indices)\n",
    "print(num_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9123e40f-ab16-412a-a54d-8703394aa598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1109, 1193, 1147, ..., 1126, 1888, 1224], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_v_indices + num_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b40760bc-0616-4f70-b72a-1522c1fd20bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain rating pairs : 64000\n",
      "\tValid rating pairs : 16000\n",
      "\tTest rating pairs  : 20000\n"
     ]
    }
   ],
   "source": [
    "# reindex u and v, v nodes start after u (v노드는 u노드 다음으로 인덱스를 부여함)\n",
    "train_v_indices += num_user\n",
    "val_v_indices += num_user\n",
    "test_v_indices += num_user\n",
    "\n",
    "train_rating_pairs  = (th.LongTensor(train_u_indices), th.LongTensor(train_v_indices))\n",
    "valid_rating_pairs  = (th.LongTensor(val_u_indices), th.LongTensor(val_v_indices))\n",
    "test_rating_pairs   = (th.LongTensor(test_u_indices), th.LongTensor(test_v_indices))\n",
    "train_rating_values = th.FloatTensor(train_labels)\n",
    "valid_rating_values = th.FloatTensor(val_labels)\n",
    "test_rating_values  = th.FloatTensor(test_labels)\n",
    "\n",
    "print(\"\\tTrain rating pairs : {}\".format(len(train_labels)))\n",
    "print(\"\\tValid rating pairs : {}\".format(len(val_labels)))\n",
    "print(\"\\tTest rating pairs  : {}\".format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f3cc1210-ea7b-447e-8bed-47733388bcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([562, 269, 478,  ..., 748, 477, 715]),\n",
       " tensor([1109, 1193, 1147,  ..., 1126, 1888, 1224]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rating_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a8441f5-cd52-4da5-9bca-a618327a0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dgl graph object, which is homogeneous and bidirectional and contains only training edges\n",
    "train_graph = dgl.graph((th.cat([train_rating_pairs[0], train_rating_pairs[1]]), \n",
    "                         th.cat([train_rating_pairs[1], train_rating_pairs[0]])))\n",
    "train_graph.edata['etype'] = th.cat([train_rating_values, train_rating_values]).to(th.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03ef4b6c-7c69-43b3-9cd2-4016015deec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=2625, num_edges=128000,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={'etype': Scheme(shape=(), dtype=torch.int64)})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a9d9d-6409-4868-af6b-87205f715b77",
   "metadata": {},
   "source": [
    "- class 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47221e4f-912c-4fc4-a050-0b0043450868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_raw_user_data():\n",
    "    \"\"\"In MovieLens, the user attributes file have the following formats:\n",
    "\n",
    "    ml-100k:\n",
    "    user id | age | gender | occupation | zip code\n",
    "\n",
    "    ml-1m:\n",
    "    UserID::Gender::Age::Occupation::Zip-code\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    user_data : pd.DataFrame\n",
    "    \"\"\"\n",
    "    if _data_name == 'ml-100k':\n",
    "        user_data = pd.read_csv(os.path.join(_dir, 'u.user'), sep='|', header=None,\n",
    "                                names=['id', 'age', 'gender', 'occupation', 'zip_code'], engine='python')\n",
    "    elif _data_name == 'ml-1m':\n",
    "        user_data = pd.read_csv(os.path.join(_dir, 'users.dat'), sep='::', header=None,\n",
    "                                names=['id', 'gender', 'age', 'occupation', 'zip_code'], engine='python')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return user_data\n",
    "\n",
    "def _load_raw_movie_data():\n",
    "    \"\"\"In MovieLens, the movie attributes may have the following formats:\n",
    "\n",
    "    In ml-100k:\n",
    "\n",
    "    movie id | movie title | release date | video release date | IMDb URL | [genres]\n",
    "\n",
    "    In ml-1m, ml-10m:\n",
    "\n",
    "    MovieID::Title (Release Year)::Genres\n",
    "\n",
    "    Also, Genres are separated by |, e.g., Adventure|Animation|Children|Comedy|Fantasy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    movie_data : pd.DataFrame\n",
    "        For ml-100k, the column name is ['id', 'title', 'release_date', 'video_release_date', 'url'] + [GENRES (19)]]\n",
    "        For ml-1m, the column name is ['id', 'title'] + [GENRES (18/20)]]\n",
    "    \"\"\"\n",
    "    if _data_name == 'ml-100k':\n",
    "        GENRES = GENRES_ML_100K\n",
    "    elif _data_name == 'ml-1m':\n",
    "        GENRES = GENRES_ML_1M\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if _data_name == 'ml-100k':\n",
    "        file_path = os.path.join(_dir, 'u.item')\n",
    "        movie_data = pd.read_csv(file_path, sep='|', header=None,\n",
    "                                      names=['id', 'title', 'release_date', 'video_release_date', 'url'] + GENRES,\n",
    "                                      engine='python')\n",
    "    elif _data_name == 'ml-1m':\n",
    "        file_path = os.path.join(_dir, 'movies.dat')\n",
    "        movie_data = pd.read_csv(file_path, sep='::', header=None,\n",
    "                                 names=['id', 'title', 'genres'], engine='python')\n",
    "        genre_map = {ele: i for i, ele in enumerate(GENRES)}\n",
    "        genre_map['Children\\'s'] = genre_map['Children']\n",
    "        genre_map['Childrens'] = genre_map['Children']\n",
    "        movie_genres = np.zeros(shape=(movie_data.shape[0], len(GENRES)), dtype=np.float32)\n",
    "        for i, genres in enumerate(movie_data['genres']):\n",
    "            for ele in genres.split('|'):\n",
    "                if ele in genre_map:\n",
    "                    movie_genres[i, genre_map[ele]] = 1.0\n",
    "                else:\n",
    "                    print('genres not found, filled with unknown: {}'.format(genres))\n",
    "                    movie_genres[i, genre_map['unknown']] = 1.0\n",
    "        for idx, genre_name in enumerate(GENRES):\n",
    "            assert idx == genre_map[genre_name]\n",
    "            movie_data[genre_name] = movie_genres[:, idx]\n",
    "        movie_data = movie_data.drop(columns=[\"genres\"])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return movie_data\n",
    "\n",
    "def _load_raw_rates(self, file_path, sep):\n",
    "    \"\"\"In MovieLens, the rates have the following format\n",
    "\n",
    "    ml-100k\n",
    "    user id \\t movie id \\t rating \\t timestamp\n",
    "\n",
    "    ml-1m/10m\n",
    "    UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "    timestamp is unix timestamp and can be converted by pd.to_datetime(X, unit='s')\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rating_data : pd.DataFrame\n",
    "    \"\"\"\n",
    "    rating_data = pd.read_csv(\n",
    "        file_path, sep=sep, header=None,\n",
    "        names=['user_id', 'movie_id', 'rating', 'timestamp'],\n",
    "        dtype={'user_id': np.int32, 'movie_id' : np.int32,\n",
    "               'ratings': np.float32, 'timestamp': np.int64}, engine='python')\n",
    "    return rating_data\n",
    "\n",
    "def _drop_unseen_nodes(self, data_df, col_name, reserved_ids_set):\n",
    "    data_df = data_df[data_df[col_name].isin(reserved_ids_set)]\n",
    "    data_df.reset_index(drop=True, inplace=True)\n",
    "    return data_df\n",
    "\n",
    "def _generate_pair_value(self, rating_data):\n",
    "    rating_pairs = (np.array([_global_user_id_map[ele] for ele in rating_data[\"user_id\"]],\n",
    "                             dtype=np.int32),\n",
    "                    np.array([_global_movie_id_map[ele] for ele in rating_data[\"movie_id\"]],\n",
    "                             dtype=np.int32))\n",
    "    # label ranges from 0. to 4.\n",
    "    rating_values = rating_data[\"rating\"].values.astype(np.float32) - 1.\n",
    "    return rating_pairs[0], rating_pairs[1], rating_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2944fac-55a6-483e-bee4-c4c53a93ad78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b1a26-c71e-42fc-b9a2-10c7ca719016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
