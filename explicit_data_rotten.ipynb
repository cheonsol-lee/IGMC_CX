{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e1b2cd-68a7-4852-b597-305881207c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training epoch took: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"  Training epoch took: {:}\".format(format_time(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e55034-0d6a-4f12-9850-33036fac7819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Rotten Tomato dataset\"\"\"\n",
    "\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "\n",
    "import dgl \n",
    "from dgl.data.utils import download, extract_archive, get_download_dir\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df84242a-d6e0-415c-b6cd-62c77dc96cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RottenTomato(object):\n",
    "    def __init__(self, data_type, label_type, path, testing=False, \n",
    "                 test_ratio=0.1, valid_ratio=0.2):\n",
    "      \n",
    "        print(f\"Data_type: {data_type}\")\n",
    "        print(f\"Label_type: {label_type}\")\n",
    "        (\n",
    "            num_user, num_movie, adj_train, \n",
    "            train_labels, train_u_indices, train_v_indices,\n",
    "            val_labels, val_u_indices, val_v_indices, \n",
    "            test_labels, test_u_indices, test_v_indices, \n",
    "            class_values\n",
    "        ) = load_official_trainvaltest_split(data_type, label_type, path, testing, None, None, 1.0)\n",
    "            \n",
    "        self._num_user = num_user\n",
    "        self._num_movie = num_movie\n",
    "\n",
    "        # reindex u and v, v nodes start after u\n",
    "        train_v_indices += self.num_user\n",
    "        val_v_indices += self.num_user\n",
    "        test_v_indices += self.num_user\n",
    "\n",
    "        self.train_rating_pairs = (th.LongTensor(train_u_indices), th.LongTensor(train_v_indices))\n",
    "        self.valid_rating_pairs = (th.LongTensor(val_u_indices), th.LongTensor(val_v_indices))\n",
    "        self.test_rating_pairs = (th.LongTensor(test_u_indices), th.LongTensor(test_v_indices))\n",
    "        self.train_rating_values = th.FloatTensor(train_labels)\n",
    "        self.valid_rating_values = th.FloatTensor(val_labels)\n",
    "        self.test_rating_values = th.FloatTensor(test_labels)\n",
    "\n",
    "        print(\"\\tTrain rating pairs : {}\".format(len(train_labels)))\n",
    "        print(\"\\tValid rating pairs : {}\".format(len(val_labels)))\n",
    "        print(\"\\tTest rating pairs  : {}\".format(len(test_labels)))\n",
    "\n",
    "        # build dgl graph object, which is homogeneous and bidirectional and contains only training edges\n",
    "        self.train_graph = dgl.graph((th.cat([self.train_rating_pairs[0], self.train_rating_pairs[1]]), \n",
    "                                      th.cat([self.train_rating_pairs[1], self.train_rating_pairs[0]])))\n",
    "        self.train_graph.edata['etype'] = th.cat([self.train_rating_values, self.train_rating_values]).to(th.long)\n",
    "\n",
    "    @property\n",
    "    def num_rating(self):\n",
    "        return self._rating.size\n",
    "\n",
    "    @property\n",
    "    def num_user(self):\n",
    "        return self._num_user\n",
    "\n",
    "    @property\n",
    "    def num_movie(self):\n",
    "        return self._num_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "398e2e59-021d-461f-ab78-d2294b976034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# For automatic dataset downloading\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df99d352-36d8-4cca-8150-724f4d9c732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data(data):\n",
    "    \"\"\"\n",
    "    Map data to proper indices in case they are not in a continues [0, N) range\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.int32 arrays\n",
    "    Returns\n",
    "    -------\n",
    "    mapped_data : np.int32 arrays\n",
    "    n : length of mapped_data\n",
    "    \"\"\"\n",
    "    uniq = list(set(data))\n",
    "\n",
    "    id_dict = {old: new for new, old in enumerate(sorted(uniq))}\n",
    "    data = np.array([id_dict[x] for x in data])\n",
    "    n = len(uniq)\n",
    "\n",
    "    return data, id_dict, n\n",
    "\n",
    "def load_official_trainvaltest_split(data_type, label_type, path, testing=False, rating_map=None, post_rating_map=None, ratio=1.0):\n",
    "    dtypes = {'u_nodes': np.int16, 'v_nodes': np.int16, 'ratings': np.float16}\n",
    "    \n",
    "    # data 로드\n",
    "    if data_type=='rotten':\n",
    "        dtypes = {'u_nodes': np.int64, 'v_nodes': np.int64, 'ratings': np.float64}\n",
    "        data_train = pd.read_csv(path + 'rotten_trainset.csv', dtype=dtypes)\n",
    "        data_test  = pd.read_csv(path + 'rotten_testset.csv', dtype=dtypes)\n",
    "    elif data_type=='amazon':\n",
    "        dtypes = {'u_nodes': np.int64, 'v_nodes': np.int64, 'ratings': np.int64}\n",
    "        data_train = pd.read_csv(path + 'amazon_trainset.csv', dtype=dtypes)\n",
    "        data_test  = pd.read_csv(path + 'amazon_testset.csv', dtype=dtypes)\n",
    "    \n",
    "    # label_type에 따른 처리\n",
    "    if data_type=='rotten':\n",
    "        if label_type=='rating':\n",
    "            data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating_0.5':'ratings'}, inplace=True)\n",
    "            data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating_0.5':'ratings'}, inplace=True)\n",
    "        elif label_type=='sentiment':    \n",
    "            data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'sentiment':'ratings'}, inplace=True)\n",
    "            data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'sentiment':'ratings'}, inplace=True)    \n",
    "        elif label_type=='emotion':    \n",
    "            data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'emotion':'ratings'}, inplace=True)\n",
    "            data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'emotion':'ratings'}, inplace=True)   \n",
    "    elif data_type=='amazon':\n",
    "        if label_type=='rating':\n",
    "            data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating':'ratings'}, inplace=True)\n",
    "            data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating':'ratings'}, inplace=True)\n",
    "        elif label_type=='sentiment':    \n",
    "            data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'sentiment':'ratings'}, inplace=True)\n",
    "            data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'sentiment':'ratings'}, inplace=True)    \n",
    "        elif label_type=='emotion':    \n",
    "            data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'emotion':'ratings'}, inplace=True)\n",
    "            data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'emotion':'ratings'}, inplace=True)   \n",
    "        \n",
    "    columns = ['u_nodes','v_nodes','ratings']\n",
    "\n",
    "    data_train = data_train[columns]\n",
    "    data_test  = data_test[columns]\n",
    "    \n",
    "    data_array_train = data_train.values.tolist()\n",
    "    data_array_train = np.array(data_array_train)\n",
    "    data_array_test = data_test.values.tolist()\n",
    "    data_array_test = np.array(data_array_test)\n",
    "\n",
    "    data_array = np.concatenate([data_array_train, data_array_test], axis=0)\n",
    "\n",
    "    u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "    v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "    ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "    if rating_map is not None:\n",
    "        for i, x in enumerate(ratings):\n",
    "            ratings[i] = rating_map[x]\n",
    "\n",
    "    u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "    v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "    u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "    ratings = ratings.astype(np.float64)\n",
    "\n",
    "    u_nodes = u_nodes_ratings\n",
    "    v_nodes = v_nodes_ratings\n",
    "\n",
    "    neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "    # assumes that ratings_train contains at least one example of every rating type\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "\n",
    "    # 경고무시함\n",
    "#     for i in range(len(u_nodes)):\n",
    "#         assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges, see cf-nade code\n",
    "\n",
    "    num_train = data_array_train.shape[0]\n",
    "    num_test = data_array_test.shape[0]\n",
    "    num_val = int(np.ceil(num_train * 0.2))\n",
    "    num_train = num_train - num_val\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    # 경고 무시함\n",
    "#     for i in range(len(ratings)):\n",
    "#         assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "    idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "    pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "    pairs_nonzero_test = pairs_nonzero[num_train+num_val:]\n",
    "\n",
    "    # Internally shuffle training set (before splitting off validation set)\n",
    "    rand_idx = list(range(len(idx_nonzero_train)))\n",
    "    np.random.seed(1234)\n",
    "    np.random.shuffle(rand_idx)\n",
    "    idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "    pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "    idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "    pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "    val_idx = idx_nonzero[0:num_val]\n",
    "    train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    assert(len(test_idx) == num_test)\n",
    "\n",
    "    val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "    train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "    \n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    if post_rating_map is None:\n",
    "        rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    else:\n",
    "        rating_mx_train[train_idx] = np.array([post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "    \n",
    "    \n",
    "    return num_users, num_items, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92ac58-d531-44f6-8393-c2c1dfe4e71d",
   "metadata": {},
   "source": [
    "# 1. Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "369bc71c-f8e9-4553-871e-d157f5552882",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './raw_data/rotten_tomato/'\n",
    "data_type = 'amazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3da335bb-3680-4fcd-aa80-2611ff17c1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label_type: rating\n",
      "\tTrain rating pairs : 216328\n",
      "\tValid rating pairs : 43266\n",
      "\tTest rating pairs  : 28766\n"
     ]
    }
   ],
   "source": [
    "label_type = 'rating'\n",
    "dataset = RottenTomato(data_type, label_type, path, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4ec875b9-2337-4ed7-b44a-99177dc7fed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label_type: sentiment\n",
      "\tTrain rating pairs : 216328\n",
      "\tValid rating pairs : 43266\n",
      "\tTest rating pairs  : 28766\n"
     ]
    }
   ],
   "source": [
    "label_type = 'sentiment'\n",
    "dataset = RottenTomato((data_type, label_type, path, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "31830f1c-d5e4-48bb-97e9-ff9c0e669778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label_type: emotion\n",
      "\tTrain rating pairs : 216328\n",
      "\tValid rating pairs : 43266\n",
      "\tTest rating pairs  : 28766\n"
     ]
    }
   ],
   "source": [
    "label_type = 'emotion'\n",
    "dataset = RottenTomato((data_type, label_type, path, testing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a04e62-2003-4474-98e9-024156a6ded9",
   "metadata": {},
   "source": [
    "# 2. load_official_trainvaltest_split 함수 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b69d4a-fadd-45db-9035-aa17c4443d30",
   "metadata": {},
   "source": [
    "- 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ec2f78f-2434-43cd-9ce5-f9789d40f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'amazon'\n",
    "# data_type = 'rotten'\n",
    "\n",
    "label_type = 'emotion'\n",
    "# label_type = 'sentiment'\n",
    "\n",
    "testing = False\n",
    "rating_map = None\n",
    "post_rating_map = None\n",
    "ratio = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47efa3-5007-46f3-8ed7-73ff4b6f8536",
   "metadata": {},
   "source": [
    "- Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0b4714d-39a8-4aaf-9a6c-e6a4aeb4835e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon',\n",
       " 'amazon_testset.csv',\n",
       " 'amazon_trainset.csv',\n",
       " 'ml-100k',\n",
       " 'model',\n",
       " 'rotten_testset.csv',\n",
       " 'rotten_tomato',\n",
       " 'rotten_trainset.csv']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './raw_data/'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a01f0ad-ab99-4fa9-a6a6-9c30f5da165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type=='rotten':\n",
    "    dtypes = {'u_nodes': np.int64, 'v_nodes': np.int64, 'ratings': np.float64}\n",
    "    data_train = pd.read_csv(path + 'rotten_trainset.csv', dtype=dtypes)\n",
    "    data_test  = pd.read_csv(path + 'rotten_testset.csv', dtype=dtypes)\n",
    "elif data_type=='amazon':\n",
    "    dtypes = {'u_nodes': np.int64, 'v_nodes': np.int64, 'ratings': np.int64}\n",
    "    data_train = pd.read_csv(path + 'amazon_trainset.csv', dtype=dtypes)\n",
    "    data_test  = pd.read_csv(path + 'amazon_testset.csv', dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e45cd2e-bed5-4551-8326-0da26626f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type=='rotten':\n",
    "    if label_type=='rating':\n",
    "        data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating_0.5':'ratings'}, inplace=True)\n",
    "        data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating_0.5':'ratings'}, inplace=True)\n",
    "    elif label_type=='sentiment':    \n",
    "        data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'sentiment':'ratings'}, inplace=True)\n",
    "        data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'sentiment':'ratings'}, inplace=True)    \n",
    "    elif label_type=='emotion':    \n",
    "        data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'emotion':'ratings'}, inplace=True)\n",
    "        data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'emotion':'ratings'}, inplace=True)   \n",
    "        \n",
    "elif data_type=='amazon':\n",
    "    if label_type=='rating':\n",
    "        data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating':'ratings'}, inplace=True)\n",
    "        data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating':'ratings'}, inplace=True)\n",
    "    elif label_type=='sentiment':    \n",
    "        data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'sentiment':'ratings'}, inplace=True)\n",
    "        data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'sentiment':'ratings'}, inplace=True)    \n",
    "    elif label_type=='emotion':    \n",
    "        data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'emotion':'ratings'}, inplace=True)\n",
    "        data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'emotion':'ratings'}, inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6820c5a6-13ab-40ea-8e96-77912cdd56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['u_nodes','v_nodes','ratings']\n",
    "data_train = data_train[columns]\n",
    "data_test  = data_test[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "037e5afd-bd82-4df4-83a7-747eb1ce9144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_nodes</th>\n",
       "      <th>v_nodes</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104555</td>\n",
       "      <td>30037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80035</td>\n",
       "      <td>13314</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73255</td>\n",
       "      <td>47748</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65251</td>\n",
       "      <td>3302</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39753</td>\n",
       "      <td>40862</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   u_nodes  v_nodes  ratings\n",
       "0   104555    30037        0\n",
       "1    80035    13314        2\n",
       "2    73255    47748        2\n",
       "3    65251     3302        2\n",
       "4    39753    40862        2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94c85ace-99b9-4275-81d3-48d1b435574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 3)\n",
      "(35947, 3)\n"
     ]
    }
   ],
   "source": [
    "data_array_train = data_train.values.tolist()\n",
    "data_array_train = np.array(data_array_train)\n",
    "data_array_test = data_test.values.tolist()\n",
    "data_array_test = np.array(data_array_test)\n",
    "\n",
    "print(data_array_train.shape)\n",
    "print(data_array_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c251f672-a28f-4f6e-9431-937f6565ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = np.concatenate([data_array_train, data_array_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1cba2f3-8165-448c-8aba-ee7ab58ae90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195947, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11f5360d-0890-4ce4-a107-c76ec2876ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "if rating_map is not None:\n",
    "    for i, x in enumerate(ratings):\n",
    "        ratings[i] = rating_map[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1343ec85-eee7-4e1b-a60c-f79d842b9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 0번부터 시작하게끔 모든 인덱스를 당기기  ex) 1번 -> 0번 / 456번 -> 455번\n",
    "u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cddfb99-468a-4f23-a8a4-5bdc1102c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int32), v_nodes_ratings.astype(np.int32)\n",
    "ratings = ratings.astype(np.float64)\n",
    "\n",
    "u_nodes = u_nodes_ratings\n",
    "v_nodes = v_nodes_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985643b-77f3-45d3-9500-646632ef5004",
   "metadata": {},
   "source": [
    "- Sparse matrix (희소행렬 형태의 인접행렬 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90c2f9f3-6f75-4db8-a8a7-3d90e23a988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3659, 33898)\n"
     ]
    }
   ],
   "source": [
    "neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "# assumes that ratings_train contains at least one example of every rating type\n",
    "rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e761e23-e075-4df9-9ff2-a142aa8bb396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a84f9f4-dbe1-4e50-a519-71923aa13e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e4cc58b-6244-4264-b62d-d5f9f653e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.reshape([-1])\n",
    "\n",
    "# number of test and validation edges, see cf-nade code\n",
    "\n",
    "num_train = data_array_train.shape[0]\n",
    "num_test = data_array_test.shape[0]\n",
    "num_val = int(np.ceil(num_train * 0.2))\n",
    "num_train = num_train - num_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f943d6cd-13ce-4ba3-baab-f11205f58519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000\n",
      "32000\n",
      "35947\n"
     ]
    }
   ],
   "source": [
    "print(num_train)\n",
    "print(num_val)\n",
    "print(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b8278ab-c34f-42c3-9146-9e3f63e3c3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195947"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e6a98f5-5b2a-4bb9-af64-239bff444110",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero]) # rating index 번호"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f51034-a452-4c8e-b3ef-a5415df0293b",
   "metadata": {},
   "source": [
    "- trainset/testset 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5c07cb0-fd10-44d0-9fc5-383e025e27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(ratings)):\n",
    "#     assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "pairs_nonzero_test = pairs_nonzero[num_train+num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed549e7d-0cc6-469c-97f5-f11ccc2663e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validset 생성 전에 trainset을 섞기\n",
    "rand_idx = list(range(len(idx_nonzero_train)))\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(rand_idx)\n",
    "idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "# 다시 합치고 train/val/test 나누기\n",
    "idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "val_idx = idx_nonzero[0:num_val]\n",
    "train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "test_pairs_idx = pairs_nonzero[num_train + num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd0e99ba-ff3c-4a72-a990-381476580f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_test_idx, v_test_idx = test_pairs_idx.transpose() \n",
    "u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "# create labels\n",
    "train_labels = labels[train_idx]\n",
    "val_labels = labels[val_idx]\n",
    "test_labels = labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e71cf98b-773f-4406-aef9-4c272f32b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "    v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "    train_labels = np.hstack([train_labels, val_labels])\n",
    "    # for adjacency matrix construction\n",
    "    train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "class_values = np.sort(np.unique(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9963cc16-a783-4a9d-9286-90244c0543bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset의 인접행렬 생성\n",
    "rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "if post_rating_map is None:\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "else:\n",
    "    rating_mx_train[train_idx] = np.array([post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n",
    "    \n",
    "rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0eeb86-7ba7-4fa0-b705-e82d0c601095",
   "metadata": {},
   "source": [
    "# 3. RottenTomato 클래스 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "591ed2fa-969c-4819-97fb-badd8973d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\n"
     ]
    }
   ],
   "source": [
    "# MovieLens(\"ml-100k\", testing=True)\n",
    "data_type, label_type = 'amazon', 'rating'\n",
    "testing = False\n",
    "test_ratio = 0.1\n",
    "valid_ratio = 0.2\n",
    "    \n",
    "print(\"Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\")\n",
    "(\n",
    "    num_user, num_item, adj_train, \n",
    "    train_labels, train_u_indices, train_v_indices,\n",
    "    val_labels, val_u_indices, val_v_indices, \n",
    "    test_labels, test_u_indices, test_v_indices, \n",
    "    class_values\n",
    ") = load_official_trainvaltest_split(data_type, label_type, path, testing, None, None, 1.0)\n",
    "\n",
    "_num_user = num_user\n",
    "_num_movie = num_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d865876-102f-453c-986c-c15fb7240f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3659\n",
      "33898\n"
     ]
    }
   ],
   "source": [
    "print(_num_user)\n",
    "print(_num_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d2e45cb-d851-4f5c-9ea2-ec1b46fc02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# property 할당(동일한 이름으로 사용)\n",
    "# num_rating = _rating.size  #GCMC에서 사용\n",
    "num_user = _num_user\n",
    "num_movie = _num_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b77b80ce-4a9e-4209-a61d-727daffa2a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4017 25711 16489 ... 17970 15017 27717]\n",
      "[ 7269  3705 30667 ...  8334 29362 16697]\n",
      "[14635 26642  3973 ... 33773 14558 29084]\n",
      "3659\n"
     ]
    }
   ],
   "source": [
    "print(train_v_indices)\n",
    "print(val_v_indices)\n",
    "print(test_v_indices)\n",
    "print(num_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9123e40f-ab16-412a-a54d-8703394aa598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7676, 29370, 20148, ..., 21629, 18676, 31376], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_v_indices + num_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b40760bc-0616-4f70-b72a-1522c1fd20bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain rating pairs : 128000\n",
      "\tValid rating pairs : 32000\n",
      "\tTest rating pairs  : 35947\n"
     ]
    }
   ],
   "source": [
    "# reindex u and v, v nodes start after u (v노드는 u노드 다음으로 인덱스를 부여함)\n",
    "train_v_indices += num_user\n",
    "val_v_indices += num_user\n",
    "test_v_indices += num_user\n",
    "\n",
    "train_rating_pairs  = (th.LongTensor(train_u_indices), th.LongTensor(train_v_indices))\n",
    "valid_rating_pairs  = (th.LongTensor(val_u_indices), th.LongTensor(val_v_indices))\n",
    "test_rating_pairs   = (th.LongTensor(test_u_indices), th.LongTensor(test_v_indices))\n",
    "train_rating_values = th.FloatTensor(train_labels)\n",
    "valid_rating_values = th.FloatTensor(val_labels)\n",
    "test_rating_values  = th.FloatTensor(test_labels)\n",
    "\n",
    "print(\"\\tTrain rating pairs : {}\".format(len(train_labels)))\n",
    "print(\"\\tValid rating pairs : {}\".format(len(val_labels)))\n",
    "print(\"\\tTest rating pairs  : {}\".format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3cc1210-ea7b-447e-8bed-47733388bcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2235,  345, 3522,  ..., 1470, 2738, 2813]),\n",
       " tensor([ 7676, 29370, 20148,  ..., 21629, 18676, 31376]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rating_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a8441f5-cd52-4da5-9bca-a618327a0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dgl graph object, which is homogeneous and bidirectional and contains only training edges\n",
    "train_graph = dgl.graph((th.cat([train_rating_pairs[0], train_rating_pairs[1]]), \n",
    "                         th.cat([train_rating_pairs[1], train_rating_pairs[0]])))\n",
    "train_graph.edata['etype'] = th.cat([train_rating_values, train_rating_values]).to(th.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03ef4b6c-7c69-43b3-9cd2-4016015deec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=37557, num_edges=256000,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={'etype': Scheme(shape=(), dtype=torch.int64)})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2944fac-55a6-483e-bee4-c4c53a93ad78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128000])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rating_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0236bf50-2017-4d41-93ba-d874888de0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
