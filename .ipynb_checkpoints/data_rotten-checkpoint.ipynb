{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e1b2cd-68a7-4852-b597-305881207c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training epoch took: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"  Training epoch took: {:}\".format(format_time(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e55034-0d6a-4f12-9850-33036fac7819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Rotten Tomato dataset\"\"\"\n",
    "\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "\n",
    "import dgl \n",
    "from dgl.data.utils import download, extract_archive, get_download_dir\n",
    "from refex import extract_refex_feature\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df84242a-d6e0-415c-b6cd-62c77dc96cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RottenTomato(object):\n",
    "    def __init__(self, path, testing=False, \n",
    "                 test_ratio=0.1, valid_ratio=0.2):\n",
    "      \n",
    "        print(\"Using RottenTomato dataset split train/test with 20% validation set size...\")\n",
    "        (\n",
    "            num_user, num_movie, adj_train, train_labels, train_u_indices, train_v_indices,\n",
    "            val_labels, val_u_indices, val_v_indices, test_labels, test_u_indices, \n",
    "            test_v_indices, class_values\n",
    "        ) = load_official_trainvaltest_split(testing, None, None, 1.0)\n",
    "            \n",
    "        self._num_user = num_user\n",
    "        self._num_movie = num_movie\n",
    "\n",
    "        # reindex u and v, v nodes start after u\n",
    "        train_v_indices += self.num_user\n",
    "        val_v_indices += self.num_user\n",
    "        test_v_indices += self.num_user\n",
    "\n",
    "        self.train_rating_pairs = (th.LongTensor(train_u_indices), th.LongTensor(train_v_indices))\n",
    "        self.valid_rating_pairs = (th.LongTensor(val_u_indices), th.LongTensor(val_v_indices))\n",
    "        self.test_rating_pairs = (th.LongTensor(test_u_indices), th.LongTensor(test_v_indices))\n",
    "        self.train_rating_values = th.FloatTensor(train_labels)\n",
    "        self.valid_rating_values = th.FloatTensor(val_labels)\n",
    "        self.test_rating_values = th.FloatTensor(test_labels)\n",
    "\n",
    "        print(\"\\tTrain rating pairs : {}\".format(len(train_labels)))\n",
    "        print(\"\\tValid rating pairs : {}\".format(len(val_labels)))\n",
    "        print(\"\\tTest rating pairs  : {}\".format(len(test_labels)))\n",
    "\n",
    "        # build dgl graph object, which is homogeneous and bidirectional and contains only training edges\n",
    "        self.train_graph = dgl.graph((th.cat([self.train_rating_pairs[0], self.train_rating_pairs[1]]), \n",
    "                                      th.cat([self.train_rating_pairs[1], self.train_rating_pairs[0]])))\n",
    "        self.train_graph.edata['etype'] = th.cat([self.train_rating_values, self.train_rating_values]).to(th.long)\n",
    "\n",
    "    @property\n",
    "    def num_rating(self):\n",
    "        return self._rating.size\n",
    "\n",
    "    @property\n",
    "    def num_user(self):\n",
    "        return self._num_user\n",
    "\n",
    "    @property\n",
    "    def num_movie(self):\n",
    "        return self._num_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398e2e59-021d-461f-ab78-d2294b976034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# For automatic dataset downloading\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df99d352-36d8-4cca-8150-724f4d9c732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data(data):\n",
    "    \"\"\"\n",
    "    Map data to proper indices in case they are not in a continues [0, N) range\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.int32 arrays\n",
    "    Returns\n",
    "    -------\n",
    "    mapped_data : np.int32 arrays\n",
    "    n : length of mapped_data\n",
    "    \"\"\"\n",
    "    uniq = list(set(data))\n",
    "\n",
    "    id_dict = {old: new for new, old in enumerate(sorted(uniq))}\n",
    "    data = np.array([id_dict[x] for x in data])\n",
    "    n = len(uniq)\n",
    "\n",
    "    return data, id_dict, n\n",
    "\n",
    "def load_official_trainvaltest_split(testing=False, rating_map=None, post_rating_map=None, ratio=1.0):\n",
    "    dtypes = {'u_nodes': np.int16, 'v_nodes': np.int16, 'ratings': np.float16}\n",
    "    \n",
    "    data_train = pd.read_csv(path + 's_trainset.csv', dtype=dtypes)\n",
    "    data_test  = pd.read_csv(path + 's_testset.csv', dtype=dtypes)\n",
    "\n",
    "    data_train.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating_0.5':'ratings'}, inplace=True)\n",
    "    data_test.rename(columns={f'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating_0.5':'ratings'}, inplace=True)\n",
    "    \n",
    "    columns = ['u_nodes','v_nodes','ratings','review_score','sentiment','emotion','review_date','origin_rating_0.5','review_content']\n",
    "#     data_train = pd.read_csv(path + 'trainset_filtered.csv', dtype=dtypes)\n",
    "#     data_test  = pd.read_csv(path + 'testset_filtered.csv', dtype=dtypes)\n",
    "        \n",
    "#     data_train.rename(columns={'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating':'ratings'}, inplace=True)\n",
    "#     data_test.rename(columns={'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating':'ratings'}, inplace=True)\n",
    "    \n",
    "#     columns = ['u_nodes','v_nodes','ratings','review_score','sentiment','emotion','review_date','origin_rating','review_content']\n",
    "    data_train = data_train[columns]\n",
    "    data_test  = data_test[columns]\n",
    "    \n",
    "    data_array_train = data_train.values.tolist()\n",
    "    data_array_train = np.array(data_array_train)\n",
    "    data_array_test = data_test.values.tolist()\n",
    "    data_array_test = np.array(data_array_test)\n",
    "\n",
    "    data_array = np.concatenate([data_array_train, data_array_test], axis=0)\n",
    "\n",
    "    u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "    v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "    ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "    if rating_map is not None:\n",
    "        for i, x in enumerate(ratings):\n",
    "            ratings[i] = rating_map[x]\n",
    "\n",
    "    u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "    v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "    u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "    ratings = ratings.astype(np.float64)\n",
    "\n",
    "    u_nodes = u_nodes_ratings\n",
    "    v_nodes = v_nodes_ratings\n",
    "\n",
    "    neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "    # assumes that ratings_train contains at least one example of every rating type\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "\n",
    "    # 경고무시함\n",
    "#     for i in range(len(u_nodes)):\n",
    "#         assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges, see cf-nade code\n",
    "\n",
    "    num_train = data_array_train.shape[0]\n",
    "    num_test = data_array_test.shape[0]\n",
    "    num_val = int(np.ceil(num_train * 0.2))\n",
    "    num_train = num_train - num_val\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    # 경고 무시함\n",
    "#     for i in range(len(ratings)):\n",
    "#         assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "    idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "    pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "    pairs_nonzero_test = pairs_nonzero[num_train+num_val:]\n",
    "\n",
    "    # Internally shuffle training set (before splitting off validation set)\n",
    "    rand_idx = list(range(len(idx_nonzero_train)))\n",
    "    np.random.seed(1234)\n",
    "    np.random.shuffle(rand_idx)\n",
    "    idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "    pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "    idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "    pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "    val_idx = idx_nonzero[0:num_val]\n",
    "    train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    assert(len(test_idx) == num_test)\n",
    "\n",
    "    val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "    train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "    \n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    if post_rating_map is None:\n",
    "        rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    else:\n",
    "        rating_mx_train[train_idx] = np.array([post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "    \n",
    "    \n",
    "    return num_users, num_items, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92ac58-d531-44f6-8393-c2c1dfe4e71d",
   "metadata": {},
   "source": [
    "# 1. Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3425856-90c9-4cac-9616-9346a01d084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RottenTomato dataset split train/test with 20% validation set size...\n",
      "\tTrain rating pairs : 90174\n",
      "\tValid rating pairs : 18035\n",
      "\tTest rating pairs  : 21435\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     dataset = MovieLens(\"ml-100k\", testing=True)\n",
    "    path = './raw_data/rotten_tomato/'\n",
    "    dataset = RottenTomato(path, testing=True)\n",
    "#     dataset = RottenTomato(path, testing=False, test_ratio=0.1, valid_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a04e62-2003-4474-98e9-024156a6ded9",
   "metadata": {},
   "source": [
    "# 2. load_official_trainvaltest_split 함수 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b69d4a-fadd-45db-9035-aa17c4443d30",
   "metadata": {},
   "source": [
    "- 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec2f78f-2434-43cd-9ce5-f9789d40f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = False\n",
    "rating_map = None\n",
    "post_rating_map = None\n",
    "ratio = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47efa3-5007-46f3-8ed7-73ff4b6f8536",
   "metadata": {},
   "source": [
    "- Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b4714d-39a8-4aaf-9a6c-e6a4aeb4835e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_rating(sentiment,emotion).csv',\n",
       " 'movie_info.csv',\n",
       " 's_testset.csv',\n",
       " 's_testset_removed.csv',\n",
       " 's_trainset.csv',\n",
       " 'testset_filtered.csv',\n",
       " 'testset_removed.csv',\n",
       " 'trainset_filtered.csv',\n",
       " 'user_info.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './raw_data/rotten_tomato/'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a01f0ad-ab99-4fa9-a6a6-9c30f5da165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'u_nodes': np.int64, 'v_nodes': np.int64, 'ratings': np.float64}\n",
    "data_train = pd.read_csv(path + 'trainset_filtered.csv', dtype=dtypes)\n",
    "data_test  = pd.read_csv(path + 'testset_filtered.csv', dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e45cd2e-bed5-4551-8326-0da26626f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.rename(columns={'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating':'ratings'}, inplace=True)\n",
    "data_test.rename(columns={'user_id':'u_nodes', 'movie_id':'v_nodes', 'rating':'ratings'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6820c5a6-13ab-40ea-8e96-77912cdd56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['u_nodes','v_nodes','ratings','review_score','sentiment','emotion','review_date','review_content']\n",
    "data_train = data_train[columns]\n",
    "data_test  = data_test[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "037e5afd-bd82-4df4-83a7-747eb1ce9144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_nodes</th>\n",
       "      <th>v_nodes</th>\n",
       "      <th>ratings</th>\n",
       "      <th>review_score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>emotion</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7403</td>\n",
       "      <td>2494</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1800-01-01</td>\n",
       "      <td>A grimly seductive end-of-the-world thriller, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6220</td>\n",
       "      <td>970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1800-01-01</td>\n",
       "      <td>All in all, AIrborne is not bad for what it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9296</td>\n",
       "      <td>7594</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1800-01-01</td>\n",
       "      <td>It's exciting to see a British horror film wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7403</td>\n",
       "      <td>8800</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.70</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1800-01-01</td>\n",
       "      <td>Coming out from behind Spike Lee's camera, Ern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2343</td>\n",
       "      <td>11205</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1800-01-01</td>\n",
       "      <td>It's the sort of film that can only be watched...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   u_nodes  v_nodes  ratings  review_score  sentiment  emotion review_date  \\\n",
       "0     7403     2494      5.0          1.00          3        4  1800-01-01   \n",
       "1     6220      970      1.0          0.20          1        2  1800-01-01   \n",
       "2     9296     7594      3.0          0.60          4        2  1800-01-01   \n",
       "3     7403     8800      3.5          0.70          4        2  1800-01-01   \n",
       "4     2343    11205      1.5          0.25          0        5  1800-01-01   \n",
       "\n",
       "                                      review_content  \n",
       "0  A grimly seductive end-of-the-world thriller, ...  \n",
       "1  All in all, AIrborne is not bad for what it is...  \n",
       "2  It's exciting to see a British horror film wit...  \n",
       "3  Coming out from behind Spike Lee's camera, Ern...  \n",
       "4  It's the sort of film that can only be watched...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94c85ace-99b9-4275-81d3-48d1b435574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216328, 8)\n",
      "(28766, 8)\n"
     ]
    }
   ],
   "source": [
    "data_array_train = data_train.values.tolist()\n",
    "data_array_train = np.array(data_array_train)\n",
    "data_array_test = data_test.values.tolist()\n",
    "data_array_test = np.array(data_array_test)\n",
    "\n",
    "print(data_array_train.shape)\n",
    "print(data_array_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c251f672-a28f-4f6e-9431-937f6565ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = np.concatenate([data_array_train, data_array_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1cba2f3-8165-448c-8aba-ee7ab58ae90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(245094, 8)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11f5360d-0890-4ce4-a107-c76ec2876ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "if rating_map is not None:\n",
    "    for i, x in enumerate(ratings):\n",
    "        ratings[i] = rating_map[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1343ec85-eee7-4e1b-a60c-f79d842b9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 0번부터 시작하게끔 모든 인덱스를 당기기  ex) 1번 -> 0번 / 456번 -> 455번\n",
    "u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3cddfb99-468a-4f23-a8a4-5bdc1102c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int32), v_nodes_ratings.astype(np.int32)\n",
    "ratings = ratings.astype(np.float64)\n",
    "\n",
    "u_nodes = u_nodes_ratings\n",
    "v_nodes = v_nodes_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985643b-77f3-45d3-9500-646632ef5004",
   "metadata": {},
   "source": [
    "- Sparse matrix (희소행렬 형태의 인접행렬 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90c2f9f3-6f75-4db8-a8a7-3d90e23a988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1112, 8521)\n"
     ]
    }
   ],
   "source": [
    "neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "# assumes that ratings_train contains at least one example of every rating type\n",
    "rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "375ef7b1-7bf0-4ea1-ace1-89b08c9f8102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       ...,\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels # user,item의 rating matrix를 -1로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a84f9f4-dbe1-4e50-a519-71923aa13e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리가 터지는 부분!!!!!\n",
    "labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00a4d141-9347-4c7e-a33d-a3afbbc47883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평점이 제대로 들어갔는지 점검\n",
    "error_index = list()\n",
    "for i in range(len(u_nodes)):\n",
    "    if labels[u_nodes[i], v_nodes[i]] != rating_dict[ratings[i]]:\n",
    "            error_index.append(i)\n",
    "    assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "222a60ec-a1a4-443a-89c5-76206092db58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e4cc58b-6244-4264-b62d-d5f9f653e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.reshape([-1])\n",
    "\n",
    "# number of test and validation edges, see cf-nade code\n",
    "\n",
    "num_train = data_array_train.shape[0]\n",
    "num_test = data_array_test.shape[0]\n",
    "num_val = int(np.ceil(num_train * 0.2))\n",
    "num_train = num_train - num_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f943d6cd-13ce-4ba3-baab-f11205f58519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106124\n",
      "26531\n",
      "21115\n"
     ]
    }
   ],
   "source": [
    "print(num_train)\n",
    "print(num_val)\n",
    "print(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b8278ab-c34f-42c3-9146-9e3f63e3c3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153770"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e6a98f5-5b2a-4bb9-af64-239bff444110",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero]) # rating index 번호"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f51034-a452-4c8e-b3ef-a5415df0293b",
   "metadata": {},
   "source": [
    "- trainset/testset 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5c07cb0-fd10-44d0-9fc5-383e025e27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ratings)):\n",
    "    assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "pairs_nonzero_test = pairs_nonzero[num_train+num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed549e7d-0cc6-469c-97f5-f11ccc2663e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validset 생성 전에 trainset을 섞기\n",
    "rand_idx = list(range(len(idx_nonzero_train)))\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(rand_idx)\n",
    "idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "val_idx = idx_nonzero[0:num_val]\n",
    "train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "assert(len(test_idx) == num_test)\n",
    "\n",
    "val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "test_pairs_idx = pairs_nonzero[num_train + num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd0e99ba-ff3c-4a72-a990-381476580f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_test_idx, v_test_idx = test_pairs_idx.transpose() # 2 x 20000으로 변경\n",
    "u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "# create labels\n",
    "train_labels = labels[train_idx]\n",
    "val_labels = labels[val_idx]\n",
    "test_labels = labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e71cf98b-773f-4406-aef9-4c272f32b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "    v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "    train_labels = np.hstack([train_labels, val_labels])\n",
    "    # for adjacency matrix construction\n",
    "    train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "class_values = np.sort(np.unique(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9963cc16-a783-4a9d-9286-90244c0543bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset의 인접행렬 생성\n",
    "rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "if post_rating_map is None:\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "else:\n",
    "    rating_mx_train[train_idx] = np.array([post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n",
    "    \n",
    "rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381cdccb-e6e1-4ae2-a7f6-6368543b36f0",
   "metadata": {},
   "source": [
    "- Movie Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7fa87fe-78fe-4423-8871-d4367e30d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # movie features (genres)\n",
    "# sep = r'|'\n",
    "# movie_file = 'raw_data/' + dataset + '/u.item'\n",
    "# movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "#                  'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "#                  'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "#                  'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "#                  'Thriller', 'War', 'Western']\n",
    "# movie_df = pd.read_csv(movie_file, sep=sep, header=None, \n",
    "#                        names=movie_headers, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e59a689-5b26-4951-a8ad-b5a79accb33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "763c9a0a-1ecc-4cef-9b3a-ee1a51431a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genre_headers = movie_df.columns.values[6:]\n",
    "# num_genres = genre_headers.shape[0]\n",
    "\n",
    "# v_features = np.zeros((num_items, num_genres), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f119caa-7f6e-4f71-944e-2ed804ec675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "282e7204-b1b5-4301-9304-d3d23a3e363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # g_vec : movie_id별 장르 벡터\n",
    "# for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "#     # check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "#     if movie_id in v_dict.keys():\n",
    "#         v_features[v_dict[movie_id], :] = g_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6502aca8-41f3-4f68-b67f-30f9eafc058a",
   "metadata": {},
   "source": [
    "- User features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa74c7b3-5bcf-4272-9c94-ee68ff55ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # user features\n",
    "# sep = r'|'\n",
    "# users_file = 'raw_data/' + dataset + '/u.user'\n",
    "# users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "# users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "#                        names=users_headers, engine='python')\n",
    "\n",
    "# occupation = set(users_df['occupation'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35e20308-b10b-4f5e-94c2-2be6386dfcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age = users_df['age'].values\n",
    "# age_max = age.max()\n",
    "\n",
    "# gender_dict = {'M': 0., 'F': 1.}\n",
    "# occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "# num_feats = 2 + len(occupation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9043b91b-ca83-410a-ab3e-f3f4137a455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # user feature 유형: age(1), gender(1), occupation(21)   총 23개\n",
    "# num_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c1c0e8b-deed-486f-8207-0b02d64f0d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80055a9c-8161-42fd-93f5-b0e7fd14d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "# for _, row in users_df.iterrows():\n",
    "#     u_id = row['user id']\n",
    "#     if u_id in u_dict.keys():\n",
    "#         # age\n",
    "#         u_features[u_dict[u_id], 0] = row['age'] / np.float(age_max)\n",
    "#         # gender\n",
    "#         u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "#         # occupation\n",
    "#         u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03f1db78-8c41-4c28-a09c-c1bdc5dd7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_features = sp.csr_matrix(u_features)\n",
    "# v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "# print(\"User features shape: \"+str(u_features.shape))\n",
    "# print(\"Item features shape: \"+str(v_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0eeb86-7ba7-4fa0-b705-e82d0c601095",
   "metadata": {},
   "source": [
    "# 3. RottenTomato 클래스 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "591ed2fa-969c-4819-97fb-badd8973d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\n"
     ]
    }
   ],
   "source": [
    "# MovieLens(\"ml-100k\", testing=True)\n",
    "\n",
    "testing = False\n",
    "test_ratio = 0.1\n",
    "valid_ratio = 0.2\n",
    "    \n",
    "\n",
    "print(\"Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\")\n",
    "(\n",
    "    num_user, num_item, adj_train, \n",
    "    train_labels, train_u_indices, train_v_indices,\n",
    "    val_labels, val_u_indices, val_v_indices, \n",
    "    test_labels, test_u_indices, test_v_indices, \n",
    "    class_values\n",
    ") = load_dataset(testing, None, None, 1.0)\n",
    "\n",
    "_num_user = num_user\n",
    "_num_movie = num_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d865876-102f-453c-986c-c15fb7240f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821\n",
      "6603\n"
     ]
    }
   ],
   "source": [
    "print(_num_user)\n",
    "print(_num_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d2e45cb-d851-4f5c-9ea2-ec1b46fc02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# property 할당(동일한 이름으로 사용)\n",
    "# num_rating = _rating.size  #GCMC에서 사용\n",
    "num_user = _num_user\n",
    "num_movie = _num_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b77b80ce-4a9e-4209-a61d-727daffa2a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1215 2031 1625 ... 3429 3275 3238]\n",
      "[ 995 2081 5081 ... 5461 2558 1791]\n",
      "[5490 4821 6393 ... 6204 4364   58]\n",
      "821\n"
     ]
    }
   ],
   "source": [
    "print(train_v_indices)\n",
    "print(val_v_indices)\n",
    "print(test_v_indices)\n",
    "print(num_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9123e40f-ab16-412a-a54d-8703394aa598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2036, 2852, 2446, ..., 4250, 4096, 4059], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_v_indices + num_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b40760bc-0616-4f70-b72a-1522c1fd20bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain rating pairs : 89192\n",
      "\tValid rating pairs : 22298\n",
      "\tTest rating pairs  : 19507\n"
     ]
    }
   ],
   "source": [
    "# reindex u and v, v nodes start after u (v노드는 u노드 다음으로 인덱스를 부여함)\n",
    "train_v_indices += num_user\n",
    "val_v_indices += num_user\n",
    "test_v_indices += num_user\n",
    "\n",
    "train_rating_pairs  = (th.LongTensor(train_u_indices), th.LongTensor(train_v_indices))\n",
    "valid_rating_pairs  = (th.LongTensor(val_u_indices), th.LongTensor(val_v_indices))\n",
    "test_rating_pairs   = (th.LongTensor(test_u_indices), th.LongTensor(test_v_indices))\n",
    "train_rating_values = th.FloatTensor(train_labels)\n",
    "valid_rating_values = th.FloatTensor(val_labels)\n",
    "test_rating_values  = th.FloatTensor(test_labels)\n",
    "\n",
    "print(\"\\tTrain rating pairs : {}\".format(len(train_labels)))\n",
    "print(\"\\tValid rating pairs : {}\".format(len(val_labels)))\n",
    "print(\"\\tTest rating pairs  : {}\".format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3cc1210-ea7b-447e-8bed-47733388bcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([280, 354, 675,  ..., 354, 112, 394]),\n",
       " tensor([2036, 2852, 2446,  ..., 4250, 4096, 4059]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rating_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a8441f5-cd52-4da5-9bca-a618327a0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dgl graph object, which is homogeneous and bidirectional and contains only training edges\n",
    "train_graph = dgl.graph((th.cat([train_rating_pairs[0], train_rating_pairs[1]]), \n",
    "                         th.cat([train_rating_pairs[1], train_rating_pairs[0]])))\n",
    "train_graph.edata['etype'] = th.cat([train_rating_values, train_rating_values]).to(th.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03ef4b6c-7c69-43b3-9cd2-4016015deec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=7424, num_edges=178384,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={'etype': Scheme(shape=(), dtype=torch.int64)})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2944fac-55a6-483e-bee4-c4c53a93ad78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
